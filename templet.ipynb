{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019/09/22 ver.1.3\n",
    "\n",
    "## 一、文件内容\n",
    "特征选择：GBRT+RT+XGboost取平均值 选择前380个特征<br />\n",
    "单模型构建：<br />\n",
    " 1：SVR<br />\n",
    " 2：RF<br />\n",
    " 3：XGBoost<br />\n",
    " 4：KNN<br />\n",
    " 5：GBRT<br />\n",
    "Stacking：<br />\n",
    " XGboost默认\n",
    "## 二、数据集\n",
    "### 导入数据集变量<br />\n",
    "元数据集 训练集全部：dg_train →训练集 特征数据：dg_train<br />\n",
    "元数据集 测试集全部：dg_test  →测试集 特征数据：dg_test<br />\n",
    "<br />\n",
    "训练集 得分数据：score_train<br />\n",
    "测试集 序号：id_test<br />\n",
    "训练集 序号：id_train\n",
    "<br />\n",
    "### 处理数据集变量<br />\n",
    "标准化处理后的原训练集 特征数据：dg_scaled_train<br />\n",
    "标准化处理后的测试集合 特征数据：dg_scaled_test<br />\n",
    "### 特征选择结果\n",
    "特征选择后的训练集 特征数据：X<br />\n",
    "特征选择后的测试集 特征数据：X_predict<br />\n",
    "### 在X中继续划分数据<br />\n",
    "X_train：0.8比例的原训练集特征 用作训练和验证<br />\n",
    "X_test：0.2比例的原训练集特征 用作测试（模型选择）<br />\n",
    "y_train：0.8比例的原训练集分数 对应X_train<br />\n",
    "y_test：0.2比例的原训练集分数 对应X_test<br />\n",
    "\n",
    "## 三、模型信息\n",
    "\n",
    "\n",
    "变量名|模型名|超参数设置|随机数种子|训练集上预测值\n",
    ":---------:|:------------------:|:-----------------:|:-----------------:|:-------------------------:\n",
    "model_svc|支持向量回归||NA|\n",
    "model_svcRF|随机森林回归||160|\n",
    "model_xgbr|Xgboost回归||161|\n",
    "model_k_neighbor|KNN回归||NA|\n",
    "model_gradient_boosting_regressor|GBRT||162|\n",
    "\n",
    "## 四、随机数信息\n",
    "位置|使用函数|随机数种子\n",
    ":------:|:-------:|:--------:|\n",
    "特征处理|GBRT|20\n",
    "特征处理|train_test_split|21\n",
    "stacking|KFold|22\n",
    "预测|train_test_split|23\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、前処理（preprocess）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预设导入\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#机器学习导入\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import explained_variance_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_train=pd.read_csv('train_all.csv')\n",
    "dg_test=pd.read_csv('test_all.csv')\n",
    "\n",
    "#单独提取ID列和score列\n",
    "id_train=dg_train['id'].values\n",
    "id_test=dg_test['id'].values\n",
    "score_train=dg_train['SalePrice'].values\n",
    "\n",
    "dg_train=dg_train.set_index(['id'])\n",
    "dg_test=dg_test.set_index(['id'])\n",
    "\n",
    "\n",
    "del dg_train['SalePrice']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31471</th>\n",
       "      <td>0.439706</td>\n",
       "      <td>-1.563669</td>\n",
       "      <td>2.886859</td>\n",
       "      <td>2.457511</td>\n",
       "      <td>0.599518</td>\n",
       "      <td>-1.694717</td>\n",
       "      <td>-0.843369</td>\n",
       "      <td>-1.067786</td>\n",
       "      <td>-2.975376</td>\n",
       "      <td>-3.046322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>2.728746</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.819145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31472</th>\n",
       "      <td>0.511786</td>\n",
       "      <td>-1.901381</td>\n",
       "      <td>2.948915</td>\n",
       "      <td>0.880258</td>\n",
       "      <td>1.253064</td>\n",
       "      <td>-2.859232</td>\n",
       "      <td>-1.944730</td>\n",
       "      <td>-0.360820</td>\n",
       "      <td>-1.962810</td>\n",
       "      <td>-4.539415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.740336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31473</th>\n",
       "      <td>0.452382</td>\n",
       "      <td>-2.082961</td>\n",
       "      <td>-3.959528</td>\n",
       "      <td>0.669883</td>\n",
       "      <td>-0.959463</td>\n",
       "      <td>0.704863</td>\n",
       "      <td>-3.940546</td>\n",
       "      <td>-0.474106</td>\n",
       "      <td>-1.147783</td>\n",
       "      <td>0.927036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>2.728746</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.418766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31474</th>\n",
       "      <td>-0.454103</td>\n",
       "      <td>0.311770</td>\n",
       "      <td>1.112790</td>\n",
       "      <td>2.839259</td>\n",
       "      <td>-1.190529</td>\n",
       "      <td>1.093948</td>\n",
       "      <td>0.297414</td>\n",
       "      <td>-0.480660</td>\n",
       "      <td>2.969900</td>\n",
       "      <td>0.890799</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.597357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31475</th>\n",
       "      <td>-1.371399</td>\n",
       "      <td>1.319441</td>\n",
       "      <td>0.510782</td>\n",
       "      <td>0.364321</td>\n",
       "      <td>-1.876332</td>\n",
       "      <td>0.834969</td>\n",
       "      <td>-0.884610</td>\n",
       "      <td>-0.230953</td>\n",
       "      <td>2.975468</td>\n",
       "      <td>-1.446590</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.807452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31476</th>\n",
       "      <td>-2.037444</td>\n",
       "      <td>1.538298</td>\n",
       "      <td>-0.562356</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>-1.824582</td>\n",
       "      <td>-0.632189</td>\n",
       "      <td>0.647655</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>3.397825</td>\n",
       "      <td>-1.452640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.710040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31477</th>\n",
       "      <td>0.016683</td>\n",
       "      <td>-1.561418</td>\n",
       "      <td>-2.045769</td>\n",
       "      <td>-0.856644</td>\n",
       "      <td>1.407410</td>\n",
       "      <td>-0.880144</td>\n",
       "      <td>-0.391444</td>\n",
       "      <td>-1.950523</td>\n",
       "      <td>0.961449</td>\n",
       "      <td>-1.081100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.788173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31478</th>\n",
       "      <td>-0.844482</td>\n",
       "      <td>0.609301</td>\n",
       "      <td>0.248616</td>\n",
       "      <td>2.494277</td>\n",
       "      <td>-0.872695</td>\n",
       "      <td>0.352762</td>\n",
       "      <td>1.181683</td>\n",
       "      <td>0.325032</td>\n",
       "      <td>3.930930</td>\n",
       "      <td>0.354652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.551115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31479</th>\n",
       "      <td>-2.685328</td>\n",
       "      <td>3.678278</td>\n",
       "      <td>1.920848</td>\n",
       "      <td>-2.142959</td>\n",
       "      <td>-1.302532</td>\n",
       "      <td>2.120069</td>\n",
       "      <td>-2.131156</td>\n",
       "      <td>-0.407303</td>\n",
       "      <td>-1.033841</td>\n",
       "      <td>-0.307280</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.080331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31480</th>\n",
       "      <td>-2.858301</td>\n",
       "      <td>3.100422</td>\n",
       "      <td>1.303294</td>\n",
       "      <td>-1.053465</td>\n",
       "      <td>-2.900735</td>\n",
       "      <td>1.425423</td>\n",
       "      <td>-1.881723</td>\n",
       "      <td>-0.516003</td>\n",
       "      <td>2.675451</td>\n",
       "      <td>-2.262686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.115268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31481</th>\n",
       "      <td>15.096075</td>\n",
       "      <td>8.150935</td>\n",
       "      <td>-0.327636</td>\n",
       "      <td>-1.162020</td>\n",
       "      <td>-4.740756</td>\n",
       "      <td>-3.646530</td>\n",
       "      <td>-0.100746</td>\n",
       "      <td>-1.691091</td>\n",
       "      <td>0.187054</td>\n",
       "      <td>-0.323244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.561889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31482</th>\n",
       "      <td>-0.603253</td>\n",
       "      <td>-0.834990</td>\n",
       "      <td>-3.980623</td>\n",
       "      <td>1.184005</td>\n",
       "      <td>-1.162382</td>\n",
       "      <td>-2.082960</td>\n",
       "      <td>-0.946058</td>\n",
       "      <td>2.122627</td>\n",
       "      <td>-1.808649</td>\n",
       "      <td>1.283379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.999718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31483</th>\n",
       "      <td>0.326856</td>\n",
       "      <td>-1.382921</td>\n",
       "      <td>0.165057</td>\n",
       "      <td>-0.314814</td>\n",
       "      <td>1.998747</td>\n",
       "      <td>-1.627087</td>\n",
       "      <td>-0.724880</td>\n",
       "      <td>-0.920753</td>\n",
       "      <td>0.788397</td>\n",
       "      <td>-2.117000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.504873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31484</th>\n",
       "      <td>0.017037</td>\n",
       "      <td>-0.203779</td>\n",
       "      <td>1.539528</td>\n",
       "      <td>4.174425</td>\n",
       "      <td>-1.029229</td>\n",
       "      <td>1.323607</td>\n",
       "      <td>0.927194</td>\n",
       "      <td>-0.506042</td>\n",
       "      <td>3.179735</td>\n",
       "      <td>2.850284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.959854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31485</th>\n",
       "      <td>-0.744808</td>\n",
       "      <td>0.280091</td>\n",
       "      <td>-1.056140</td>\n",
       "      <td>0.108890</td>\n",
       "      <td>-0.746797</td>\n",
       "      <td>-0.896369</td>\n",
       "      <td>0.304458</td>\n",
       "      <td>1.116840</td>\n",
       "      <td>1.228530</td>\n",
       "      <td>-1.860296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.597357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31486</th>\n",
       "      <td>0.053471</td>\n",
       "      <td>-0.118680</td>\n",
       "      <td>0.442263</td>\n",
       "      <td>1.190692</td>\n",
       "      <td>-0.126083</td>\n",
       "      <td>0.263456</td>\n",
       "      <td>0.269391</td>\n",
       "      <td>-0.170271</td>\n",
       "      <td>0.144161</td>\n",
       "      <td>0.415800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-1.036925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31487</th>\n",
       "      <td>-2.159759</td>\n",
       "      <td>1.726808</td>\n",
       "      <td>-0.908929</td>\n",
       "      <td>-1.136735</td>\n",
       "      <td>-0.427382</td>\n",
       "      <td>-2.996133</td>\n",
       "      <td>3.084604</td>\n",
       "      <td>-0.406424</td>\n",
       "      <td>-3.529331</td>\n",
       "      <td>3.875552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.485206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31488</th>\n",
       "      <td>0.063257</td>\n",
       "      <td>-0.296385</td>\n",
       "      <td>-0.687710</td>\n",
       "      <td>-0.132802</td>\n",
       "      <td>1.706198</td>\n",
       "      <td>-0.252733</td>\n",
       "      <td>0.687358</td>\n",
       "      <td>-0.823178</td>\n",
       "      <td>0.534338</td>\n",
       "      <td>-0.089548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>3.457738</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>5.152507</td>\n",
       "      <td>4.153906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31489</th>\n",
       "      <td>1.398443</td>\n",
       "      <td>1.113861</td>\n",
       "      <td>0.271887</td>\n",
       "      <td>0.438674</td>\n",
       "      <td>4.537986</td>\n",
       "      <td>2.953063</td>\n",
       "      <td>0.211309</td>\n",
       "      <td>3.055702</td>\n",
       "      <td>0.196731</td>\n",
       "      <td>0.323103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>2.728746</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>1.163571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31490</th>\n",
       "      <td>1.724151</td>\n",
       "      <td>2.839401</td>\n",
       "      <td>1.695019</td>\n",
       "      <td>-1.041376</td>\n",
       "      <td>2.376184</td>\n",
       "      <td>2.612194</td>\n",
       "      <td>-1.274839</td>\n",
       "      <td>2.090569</td>\n",
       "      <td>-4.628080</td>\n",
       "      <td>2.759130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-1.371783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31491</th>\n",
       "      <td>0.229256</td>\n",
       "      <td>-1.064303</td>\n",
       "      <td>-1.275092</td>\n",
       "      <td>-0.242257</td>\n",
       "      <td>0.313595</td>\n",
       "      <td>0.439267</td>\n",
       "      <td>-0.670470</td>\n",
       "      <td>-0.962897</td>\n",
       "      <td>0.315726</td>\n",
       "      <td>-0.135580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.516566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31492</th>\n",
       "      <td>0.282826</td>\n",
       "      <td>-0.796914</td>\n",
       "      <td>-3.957635</td>\n",
       "      <td>0.396481</td>\n",
       "      <td>0.324399</td>\n",
       "      <td>-1.499833</td>\n",
       "      <td>0.118277</td>\n",
       "      <td>0.669640</td>\n",
       "      <td>1.488878</td>\n",
       "      <td>-1.481684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>3.457738</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.316327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31493</th>\n",
       "      <td>-4.129578</td>\n",
       "      <td>5.047759</td>\n",
       "      <td>2.744820</td>\n",
       "      <td>-3.703989</td>\n",
       "      <td>-2.720900</td>\n",
       "      <td>1.777920</td>\n",
       "      <td>-2.578515</td>\n",
       "      <td>-3.327527</td>\n",
       "      <td>-2.448705</td>\n",
       "      <td>0.937860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.463414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31494</th>\n",
       "      <td>-0.623747</td>\n",
       "      <td>0.132314</td>\n",
       "      <td>-1.413383</td>\n",
       "      <td>-0.890492</td>\n",
       "      <td>2.885039</td>\n",
       "      <td>-0.979599</td>\n",
       "      <td>1.726224</td>\n",
       "      <td>-1.625008</td>\n",
       "      <td>1.889269</td>\n",
       "      <td>-0.518085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>3.457738</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.515115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31495</th>\n",
       "      <td>-1.184088</td>\n",
       "      <td>0.411252</td>\n",
       "      <td>-1.671793</td>\n",
       "      <td>0.835051</td>\n",
       "      <td>-1.519830</td>\n",
       "      <td>-2.743938</td>\n",
       "      <td>2.150542</td>\n",
       "      <td>4.257232</td>\n",
       "      <td>0.068059</td>\n",
       "      <td>-1.439276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>1.818404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31496</th>\n",
       "      <td>-2.843578</td>\n",
       "      <td>2.515444</td>\n",
       "      <td>-0.481892</td>\n",
       "      <td>-1.210426</td>\n",
       "      <td>-1.980216</td>\n",
       "      <td>-1.169635</td>\n",
       "      <td>0.643919</td>\n",
       "      <td>0.248719</td>\n",
       "      <td>-0.552494</td>\n",
       "      <td>0.470069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>1.014213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31497</th>\n",
       "      <td>-1.001212</td>\n",
       "      <td>0.192914</td>\n",
       "      <td>-2.231358</td>\n",
       "      <td>-0.051790</td>\n",
       "      <td>1.054576</td>\n",
       "      <td>-2.614553</td>\n",
       "      <td>2.663564</td>\n",
       "      <td>0.665845</td>\n",
       "      <td>-0.666811</td>\n",
       "      <td>1.205294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.668193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31498</th>\n",
       "      <td>-2.579696</td>\n",
       "      <td>2.707308</td>\n",
       "      <td>1.109334</td>\n",
       "      <td>-0.553160</td>\n",
       "      <td>-3.069933</td>\n",
       "      <td>1.386842</td>\n",
       "      <td>-1.831149</td>\n",
       "      <td>-0.391368</td>\n",
       "      <td>3.533762</td>\n",
       "      <td>-2.600249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.781263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31499</th>\n",
       "      <td>-0.022608</td>\n",
       "      <td>-0.921215</td>\n",
       "      <td>-2.425933</td>\n",
       "      <td>0.902401</td>\n",
       "      <td>-0.878274</td>\n",
       "      <td>-0.403192</td>\n",
       "      <td>-2.011802</td>\n",
       "      <td>0.820038</td>\n",
       "      <td>-1.487129</td>\n",
       "      <td>1.152845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.683995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31500</th>\n",
       "      <td>-0.566422</td>\n",
       "      <td>0.195443</td>\n",
       "      <td>-0.749406</td>\n",
       "      <td>0.267128</td>\n",
       "      <td>-0.468553</td>\n",
       "      <td>-1.080326</td>\n",
       "      <td>0.834610</td>\n",
       "      <td>1.214428</td>\n",
       "      <td>-0.010522</td>\n",
       "      <td>-0.216712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>1.083311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62703</th>\n",
       "      <td>-1.437634</td>\n",
       "      <td>0.945722</td>\n",
       "      <td>-0.121939</td>\n",
       "      <td>-1.540579</td>\n",
       "      <td>1.301148</td>\n",
       "      <td>-1.382672</td>\n",
       "      <td>1.078372</td>\n",
       "      <td>-1.313169</td>\n",
       "      <td>0.354360</td>\n",
       "      <td>0.179527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.475639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62704</th>\n",
       "      <td>0.029939</td>\n",
       "      <td>-0.253154</td>\n",
       "      <td>1.740540</td>\n",
       "      <td>4.618038</td>\n",
       "      <td>-1.003577</td>\n",
       "      <td>1.299073</td>\n",
       "      <td>0.997021</td>\n",
       "      <td>-0.836146</td>\n",
       "      <td>2.368187</td>\n",
       "      <td>2.600290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.043125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62705</th>\n",
       "      <td>-1.100062</td>\n",
       "      <td>1.590990</td>\n",
       "      <td>-0.063471</td>\n",
       "      <td>-0.716517</td>\n",
       "      <td>1.072147</td>\n",
       "      <td>1.123003</td>\n",
       "      <td>-0.324573</td>\n",
       "      <td>-1.746938</td>\n",
       "      <td>2.660501</td>\n",
       "      <td>-0.798772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.567592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62706</th>\n",
       "      <td>0.279574</td>\n",
       "      <td>-1.583131</td>\n",
       "      <td>0.464023</td>\n",
       "      <td>-0.843588</td>\n",
       "      <td>1.411987</td>\n",
       "      <td>-1.511319</td>\n",
       "      <td>-0.862318</td>\n",
       "      <td>-1.070149</td>\n",
       "      <td>0.785330</td>\n",
       "      <td>-1.813041</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>3.457738</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.362957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62707</th>\n",
       "      <td>15.577605</td>\n",
       "      <td>8.528465</td>\n",
       "      <td>-0.178005</td>\n",
       "      <td>-1.177072</td>\n",
       "      <td>-4.668773</td>\n",
       "      <td>-3.643723</td>\n",
       "      <td>-0.126327</td>\n",
       "      <td>-1.504316</td>\n",
       "      <td>-0.223386</td>\n",
       "      <td>0.048721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.441090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62708</th>\n",
       "      <td>-2.211054</td>\n",
       "      <td>1.728702</td>\n",
       "      <td>0.485805</td>\n",
       "      <td>-2.342219</td>\n",
       "      <td>1.063866</td>\n",
       "      <td>-0.895545</td>\n",
       "      <td>0.368986</td>\n",
       "      <td>-2.320561</td>\n",
       "      <td>0.844279</td>\n",
       "      <td>-0.376983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>3.457738</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>1.089689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62709</th>\n",
       "      <td>-0.448681</td>\n",
       "      <td>0.124299</td>\n",
       "      <td>-0.542284</td>\n",
       "      <td>0.671585</td>\n",
       "      <td>-1.027195</td>\n",
       "      <td>-1.139037</td>\n",
       "      <td>0.813290</td>\n",
       "      <td>2.623503</td>\n",
       "      <td>0.097907</td>\n",
       "      <td>-1.204080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>4.045683</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.993484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62710</th>\n",
       "      <td>-1.033347</td>\n",
       "      <td>0.420579</td>\n",
       "      <td>-1.278992</td>\n",
       "      <td>0.467305</td>\n",
       "      <td>-1.186557</td>\n",
       "      <td>-1.776934</td>\n",
       "      <td>1.183527</td>\n",
       "      <td>2.573627</td>\n",
       "      <td>0.551407</td>\n",
       "      <td>-1.461805</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.395379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62711</th>\n",
       "      <td>-1.392152</td>\n",
       "      <td>0.998879</td>\n",
       "      <td>-0.634183</td>\n",
       "      <td>-0.263907</td>\n",
       "      <td>-1.002289</td>\n",
       "      <td>-2.166204</td>\n",
       "      <td>2.212399</td>\n",
       "      <td>0.386825</td>\n",
       "      <td>-2.567857</td>\n",
       "      <td>2.822265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.382235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62712</th>\n",
       "      <td>0.601817</td>\n",
       "      <td>-2.288698</td>\n",
       "      <td>3.512468</td>\n",
       "      <td>0.560522</td>\n",
       "      <td>1.393452</td>\n",
       "      <td>-3.509221</td>\n",
       "      <td>-2.525285</td>\n",
       "      <td>-0.168919</td>\n",
       "      <td>-1.815576</td>\n",
       "      <td>-4.890054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.262499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62713</th>\n",
       "      <td>0.478114</td>\n",
       "      <td>-1.606601</td>\n",
       "      <td>-2.607197</td>\n",
       "      <td>-0.099033</td>\n",
       "      <td>0.468390</td>\n",
       "      <td>1.002620</td>\n",
       "      <td>-1.681449</td>\n",
       "      <td>-1.409584</td>\n",
       "      <td>1.320308</td>\n",
       "      <td>-0.831393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.468198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62714</th>\n",
       "      <td>0.196884</td>\n",
       "      <td>1.993954</td>\n",
       "      <td>1.746401</td>\n",
       "      <td>-0.033505</td>\n",
       "      <td>5.116666</td>\n",
       "      <td>6.629706</td>\n",
       "      <td>-2.541245</td>\n",
       "      <td>9.746659</td>\n",
       "      <td>-3.881309</td>\n",
       "      <td>-1.253763</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.521494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62715</th>\n",
       "      <td>0.354896</td>\n",
       "      <td>-1.018989</td>\n",
       "      <td>3.346460</td>\n",
       "      <td>7.492345</td>\n",
       "      <td>-0.409673</td>\n",
       "      <td>0.405605</td>\n",
       "      <td>1.220715</td>\n",
       "      <td>-2.855254</td>\n",
       "      <td>-5.102304</td>\n",
       "      <td>-0.123449</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.521350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62716</th>\n",
       "      <td>-0.764503</td>\n",
       "      <td>0.473879</td>\n",
       "      <td>0.342028</td>\n",
       "      <td>2.876069</td>\n",
       "      <td>-0.778247</td>\n",
       "      <td>0.346058</td>\n",
       "      <td>1.296790</td>\n",
       "      <td>0.128945</td>\n",
       "      <td>3.586041</td>\n",
       "      <td>0.425804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.239644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62717</th>\n",
       "      <td>0.636564</td>\n",
       "      <td>-3.036431</td>\n",
       "      <td>-5.776882</td>\n",
       "      <td>1.225181</td>\n",
       "      <td>-2.041334</td>\n",
       "      <td>1.538951</td>\n",
       "      <td>-7.121466</td>\n",
       "      <td>-0.726794</td>\n",
       "      <td>-2.908502</td>\n",
       "      <td>2.625323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>1.237452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62718</th>\n",
       "      <td>0.226133</td>\n",
       "      <td>-0.648487</td>\n",
       "      <td>2.878770</td>\n",
       "      <td>7.503202</td>\n",
       "      <td>-0.741485</td>\n",
       "      <td>1.334659</td>\n",
       "      <td>1.679111</td>\n",
       "      <td>-2.244069</td>\n",
       "      <td>-1.171686</td>\n",
       "      <td>2.056508</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.845577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62719</th>\n",
       "      <td>-0.214160</td>\n",
       "      <td>-0.166043</td>\n",
       "      <td>-0.926984</td>\n",
       "      <td>0.295405</td>\n",
       "      <td>-0.172631</td>\n",
       "      <td>-0.745810</td>\n",
       "      <td>0.298867</td>\n",
       "      <td>0.665779</td>\n",
       "      <td>-0.261431</td>\n",
       "      <td>0.089380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>4.045683</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.785516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62720</th>\n",
       "      <td>0.087352</td>\n",
       "      <td>2.265803</td>\n",
       "      <td>1.616225</td>\n",
       "      <td>-0.687579</td>\n",
       "      <td>3.322434</td>\n",
       "      <td>4.497502</td>\n",
       "      <td>-1.895768</td>\n",
       "      <td>4.593814</td>\n",
       "      <td>-3.067148</td>\n",
       "      <td>0.298172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.341696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62721</th>\n",
       "      <td>0.350347</td>\n",
       "      <td>-1.187998</td>\n",
       "      <td>2.030743</td>\n",
       "      <td>2.230238</td>\n",
       "      <td>0.945138</td>\n",
       "      <td>-1.462305</td>\n",
       "      <td>-0.681232</td>\n",
       "      <td>-0.869450</td>\n",
       "      <td>-1.947126</td>\n",
       "      <td>-2.524625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.219978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62722</th>\n",
       "      <td>-2.278358</td>\n",
       "      <td>2.271208</td>\n",
       "      <td>0.743691</td>\n",
       "      <td>-0.087504</td>\n",
       "      <td>-2.892952</td>\n",
       "      <td>1.043449</td>\n",
       "      <td>-1.368816</td>\n",
       "      <td>-0.136858</td>\n",
       "      <td>4.022771</td>\n",
       "      <td>-2.517474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.640023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62723</th>\n",
       "      <td>-2.035728</td>\n",
       "      <td>1.284173</td>\n",
       "      <td>-1.730886</td>\n",
       "      <td>-0.324203</td>\n",
       "      <td>-1.488267</td>\n",
       "      <td>-2.382180</td>\n",
       "      <td>1.714596</td>\n",
       "      <td>1.186949</td>\n",
       "      <td>-0.992576</td>\n",
       "      <td>1.064436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.482549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62724</th>\n",
       "      <td>-1.935332</td>\n",
       "      <td>2.115480</td>\n",
       "      <td>1.266303</td>\n",
       "      <td>-2.110516</td>\n",
       "      <td>-0.219074</td>\n",
       "      <td>0.039476</td>\n",
       "      <td>-0.616513</td>\n",
       "      <td>-1.898827</td>\n",
       "      <td>-1.047344</td>\n",
       "      <td>0.697688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.321110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62725</th>\n",
       "      <td>1.923838</td>\n",
       "      <td>-5.525937</td>\n",
       "      <td>2.259373</td>\n",
       "      <td>-4.693867</td>\n",
       "      <td>-5.367963</td>\n",
       "      <td>6.161667</td>\n",
       "      <td>7.516888</td>\n",
       "      <td>0.890133</td>\n",
       "      <td>-2.009487</td>\n",
       "      <td>-0.595339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.140393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62726</th>\n",
       "      <td>-2.278358</td>\n",
       "      <td>2.271208</td>\n",
       "      <td>0.743691</td>\n",
       "      <td>-0.087504</td>\n",
       "      <td>-2.892952</td>\n",
       "      <td>1.043449</td>\n",
       "      <td>-1.368816</td>\n",
       "      <td>-0.136858</td>\n",
       "      <td>4.022771</td>\n",
       "      <td>-2.517474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>2.728746</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.265688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62727</th>\n",
       "      <td>0.739899</td>\n",
       "      <td>-2.760841</td>\n",
       "      <td>4.036953</td>\n",
       "      <td>-0.701828</td>\n",
       "      <td>0.852864</td>\n",
       "      <td>-3.609738</td>\n",
       "      <td>-2.869251</td>\n",
       "      <td>1.565499</td>\n",
       "      <td>1.095536</td>\n",
       "      <td>0.718243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2.170106</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>1.677551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62728</th>\n",
       "      <td>0.676224</td>\n",
       "      <td>-3.021013</td>\n",
       "      <td>-5.615820</td>\n",
       "      <td>1.116761</td>\n",
       "      <td>-1.991444</td>\n",
       "      <td>1.652350</td>\n",
       "      <td>-6.975146</td>\n",
       "      <td>-0.850474</td>\n",
       "      <td>-2.748088</td>\n",
       "      <td>2.495488</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.486269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62729</th>\n",
       "      <td>-1.197597</td>\n",
       "      <td>0.883343</td>\n",
       "      <td>-0.355466</td>\n",
       "      <td>-1.911708</td>\n",
       "      <td>2.485277</td>\n",
       "      <td>-0.575685</td>\n",
       "      <td>0.647636</td>\n",
       "      <td>-3.647590</td>\n",
       "      <td>0.820978</td>\n",
       "      <td>0.075888</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.643068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62730</th>\n",
       "      <td>-1.298736</td>\n",
       "      <td>0.641537</td>\n",
       "      <td>-1.348481</td>\n",
       "      <td>0.741420</td>\n",
       "      <td>-1.862840</td>\n",
       "      <td>-2.458872</td>\n",
       "      <td>1.856618</td>\n",
       "      <td>4.430035</td>\n",
       "      <td>0.337982</td>\n",
       "      <td>-1.975445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>0.582087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62731</th>\n",
       "      <td>-2.478450</td>\n",
       "      <td>2.236873</td>\n",
       "      <td>-0.601128</td>\n",
       "      <td>-1.404854</td>\n",
       "      <td>-1.118895</td>\n",
       "      <td>-0.842669</td>\n",
       "      <td>0.301386</td>\n",
       "      <td>-0.927082</td>\n",
       "      <td>-0.658971</td>\n",
       "      <td>0.985701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>-1.119594</td>\n",
       "      <td>3.457738</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>1.083842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62732</th>\n",
       "      <td>0.339123</td>\n",
       "      <td>-2.292142</td>\n",
       "      <td>2.877432</td>\n",
       "      <td>-2.337310</td>\n",
       "      <td>0.916067</td>\n",
       "      <td>-2.432421</td>\n",
       "      <td>-1.243648</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>1.092831</td>\n",
       "      <td>2.073484</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03826</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>-0.289206</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>-0.460807</td>\n",
       "      <td>-0.247177</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.724391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31262 rows × 455 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1         2         3         4         5         6         7  \\\n",
       "id                                                                            \n",
       "31471  0.439706 -1.563669  2.886859  2.457511  0.599518 -1.694717 -0.843369   \n",
       "31472  0.511786 -1.901381  2.948915  0.880258  1.253064 -2.859232 -1.944730   \n",
       "31473  0.452382 -2.082961 -3.959528  0.669883 -0.959463  0.704863 -3.940546   \n",
       "31474 -0.454103  0.311770  1.112790  2.839259 -1.190529  1.093948  0.297414   \n",
       "31475 -1.371399  1.319441  0.510782  0.364321 -1.876332  0.834969 -0.884610   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "62728  0.676224 -3.021013 -5.615820  1.116761 -1.991444  1.652350 -6.975146   \n",
       "62729 -1.197597  0.883343 -0.355466 -1.911708  2.485277 -0.575685  0.647636   \n",
       "62730 -1.298736  0.641537 -1.348481  0.741420 -1.862840 -2.458872  1.856618   \n",
       "62731 -2.478450  2.236873 -0.601128 -1.404854 -1.118895 -0.842669  0.301386   \n",
       "62732  0.339123 -2.292142  2.877432 -2.337310  0.916067 -2.432421 -1.243648   \n",
       "\n",
       "              8         9        10  ...      446       447       448  \\\n",
       "id                                   ...                                \n",
       "31471 -1.067786 -2.975376 -3.046322  ... -0.03826 -1.119594 -0.289206   \n",
       "31472 -0.360820 -1.962810 -4.539415  ... -0.03826  0.893181 -0.289206   \n",
       "31473 -0.474106 -1.147783  0.927036  ... -0.03826 -1.119594 -0.289206   \n",
       "31474 -0.480660  2.969900  0.890799  ... -0.03826 -1.119594 -0.289206   \n",
       "31475 -0.230953  2.975468 -1.446590  ... -0.03826  0.893181 -0.289206   \n",
       "...         ...       ...       ...  ...      ...       ...       ...   \n",
       "62728 -0.850474 -2.748088  2.495488  ... -0.03826  0.893181 -0.289206   \n",
       "62729 -3.647590  0.820978  0.075888  ... -0.03826  0.893181 -0.289206   \n",
       "62730  4.430035  0.337982 -1.975445  ... -0.03826  0.893181 -0.289206   \n",
       "62731 -0.927082 -0.658971  0.985701  ... -0.03826 -1.119594  3.457738   \n",
       "62732  0.008276  1.092831  2.073484  ... -0.03826  0.893181 -0.289206   \n",
       "\n",
       "            449       450       451       452       453       454       455  \n",
       "id                                                                           \n",
       "31471 -0.065881 -0.009764 -0.460807 -0.247177  2.728746 -0.093518  0.819145  \n",
       "31472 -0.065881 -0.009764 -0.460807 -0.247177 -0.366469 -0.093518 -0.740336  \n",
       "31473 -0.065881 -0.009764 -0.460807 -0.247177  2.728746 -0.093518 -0.418766  \n",
       "31474 -0.065881 -0.009764  2.170106 -0.247177 -0.366469 -0.093518 -0.597357  \n",
       "31475 -0.065881 -0.009764 -0.460807 -0.247177 -0.366469 -0.093518  0.807452  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "62728 -0.065881 -0.009764 -0.460807 -0.247177 -0.366469 -0.093518 -0.486269  \n",
       "62729 -0.065881 -0.009764 -0.460807 -0.247177 -0.366469 -0.093518 -0.643068  \n",
       "62730 -0.065881 -0.009764 -0.460807 -0.247177 -0.366469 -0.093518  0.582087  \n",
       "62731 -0.065881 -0.009764 -0.460807 -0.247177 -0.366469 -0.093518  1.083842  \n",
       "62732 -0.065881 -0.009764 -0.460807 -0.247177 -0.366469 -0.093518 -0.724391  \n",
       "\n",
       "[31262 rows x 455 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31262 entries, 31471 to 62732\n",
      "Columns: 455 entries, 1 to 455\n",
      "dtypes: float64(455)\n",
      "memory usage: 108.8 MB\n"
     ]
    }
   ],
   "source": [
    "dg_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',200)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "1      False\n",
      "2      False\n",
      "3      False\n",
      "4      False\n",
      "5      False\n",
      "       ...  \n",
      "451    False\n",
      "452    False\n",
      "453    False\n",
      "454    False\n",
      "455    False\n",
      "Length: 455, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "temp = dg_test.isnull().any() #列中是否存在空值\n",
    "print(type(temp))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "[-9.84418916e-17  1.12892077e-17  9.48293451e-17 -1.02054438e-16\n",
      " -6.50258366e-17  1.34567356e-16  4.69631042e-17  2.21268472e-17\n",
      "  7.36056345e-17  3.97380113e-17 -1.26439127e-17  1.06570121e-16\n",
      " -1.11537373e-16  6.32195634e-18 -1.62564592e-17 -7.76697493e-17\n",
      " -4.28989894e-17 -2.93519401e-17 -3.56738965e-17 -7.67666127e-18\n",
      "  4.83178092e-17 -1.14246782e-16  1.49017542e-16  1.29374321e-16\n",
      "  6.84125990e-17  3.52223282e-17  6.23164268e-17 -8.12822958e-18\n",
      " -2.25784155e-17 -6.66063257e-17 -1.71595958e-17 -1.11085804e-16\n",
      "  1.64822433e-17 -2.93519401e-17  3.92864430e-17  2.61909620e-17\n",
      "  3.83833063e-17  6.50258366e-17 -2.70940986e-17 -4.96725141e-17\n",
      " -1.26439127e-17  4.17700687e-17  0.00000000e+00  2.00947898e-17\n",
      " -8.08307275e-17 -7.47345553e-17  3.70286014e-17  6.77352465e-18\n",
      "  5.46397655e-17  6.52516208e-17  8.26370007e-17  2.77714511e-17\n",
      "  3.52223282e-17  3.38676232e-17 -3.58996806e-17  1.53533225e-17\n",
      "  6.90899514e-17  1.08376394e-17  6.32195634e-17 -2.66425303e-17\n",
      " -9.70871866e-17  4.87693775e-17 -3.40934074e-17  1.08376394e-17\n",
      "  1.08376394e-17  5.08014349e-19 -7.67666127e-18  6.77352465e-19\n",
      "  1.80627324e-17 -1.76111641e-17  2.64167461e-17 -3.31902708e-17\n",
      "  2.98035085e-17  1.19665602e-17 -4.15442845e-17 -9.88934599e-17\n",
      " -7.22509296e-18  1.26439127e-17  7.13477930e-17  2.59651778e-17\n",
      "  1.49017542e-17 -3.34160549e-17  4.74146725e-17 -2.98035085e-17\n",
      " -3.68028173e-17  2.52878254e-17 -5.75749595e-18  6.07359377e-17\n",
      " -5.59944704e-17  1.18536681e-17 -7.23638217e-17  1.44501859e-17\n",
      " -3.61254648e-18  1.08376394e-17  6.27679951e-17 -2.68683144e-17\n",
      " -1.44501859e-17 -6.34453476e-17 -3.38676232e-17 -3.83833063e-17\n",
      " -8.12822958e-18 -1.80627324e-18 -6.32195634e-18  6.32195634e-18\n",
      " -6.68321099e-17 -4.60599676e-17  7.13477930e-17  5.87038803e-18\n",
      " -4.04153637e-17  3.65770331e-17  4.60599676e-17 -3.27387025e-17\n",
      "  5.41881972e-18  5.08014349e-17 -1.94174373e-17 -4.59470755e-17\n",
      "  2.34815521e-17  1.35470493e-17 -3.09324292e-17 -3.52223282e-17\n",
      " -1.03860711e-17 -2.25784155e-18 -2.89003718e-17 -2.98035085e-17\n",
      " -5.73491754e-17 -7.85728859e-17  4.56083993e-17 -3.18355659e-17\n",
      "  2.34815521e-17 -4.24474211e-17  1.62564592e-17 -4.15442845e-17\n",
      " -1.89658690e-17  4.80920250e-17  2.93519401e-18 -1.98690056e-17\n",
      " -1.91916532e-17  1.17407761e-17  1.21923444e-17  7.45087711e-18\n",
      "  1.98690056e-17 -3.07066451e-17  3.52223282e-17 -2.66425303e-17\n",
      " -1.71595958e-17  7.72181810e-17 -2.64167461e-17 -8.48948423e-17\n",
      "  1.17407761e-17  2.34815521e-17 -3.70286014e-17  3.13839975e-17\n",
      "  1.62564592e-17  2.16752789e-17  4.38021261e-17  3.61254648e-18\n",
      "  4.51568310e-19  4.01895796e-17 -2.70940986e-18 -1.80627324e-17\n",
      " -1.08376394e-17  9.03136620e-18 -2.33686600e-17  1.12892077e-17\n",
      " -2.23526313e-17 -4.87693775e-17  3.07066451e-17 -5.39624130e-17\n",
      "  3.99637954e-17  3.83833063e-18  2.28041997e-17  4.78662409e-17\n",
      " -2.33686600e-17 -1.98690056e-17 -2.49491491e-17 -1.89658690e-17\n",
      " -5.59944704e-17 -4.28989894e-18  6.19777505e-17  1.30954810e-17\n",
      " -9.03136620e-18 -2.12237106e-17 -2.33686600e-17 -1.17407761e-17\n",
      " -1.91916532e-18 -2.32557680e-17  2.39331204e-17 -1.80627324e-18\n",
      " -1.39986176e-17  9.25715035e-18 -3.16097817e-18 -4.24474211e-17\n",
      " -2.03205739e-18  1.76111641e-17 -3.83833063e-17  1.29825889e-17\n",
      " -3.38676232e-17  1.62000131e-17  1.36599414e-17  2.30299838e-17\n",
      "  3.83833063e-18 -1.39986176e-17 -1.02731791e-17 -3.61254648e-18\n",
      "  4.51568310e-18 -1.71595958e-17  4.96725141e-18 -3.03961919e-17\n",
      "  7.11220088e-18  4.51568310e-19  2.82230194e-17 -3.10453213e-18\n",
      " -1.80627324e-17  3.77059539e-17  1.14020998e-17  4.96725141e-18\n",
      "  2.09979264e-17 -4.35763419e-17  8.01533750e-18  6.77352465e-19\n",
      "  4.38021261e-17 -5.87038803e-18  7.67666127e-18 -2.25784155e-17\n",
      " -1.35470493e-17 -3.52223282e-17  1.67080275e-17 -8.57979789e-18\n",
      " -1.21923444e-17  1.62564592e-17  5.41881972e-18 -2.00947898e-17\n",
      "  3.16097817e-18 -1.28696968e-17 -1.86271928e-17  1.35470493e-17\n",
      " -3.70286014e-17 -3.61254648e-18 -1.20794523e-17  3.34160549e-17\n",
      "  3.70286014e-17 -4.96725141e-18 -1.68209195e-17 -3.61254648e-18\n",
      " -2.61909620e-17 -2.89003718e-17 -6.43484842e-18 -1.19665602e-17\n",
      " -7.90244542e-18  2.70940986e-18  5.41881972e-18 -9.03136620e-19\n",
      " -1.26439127e-17  4.31247736e-17 -1.63693512e-17 -1.35470493e-17\n",
      " -6.77352465e-18 -2.48362570e-18 -1.42244018e-17  2.03205739e-17\n",
      " -1.53533225e-17 -4.92209458e-17  3.81575222e-17  2.16752789e-17\n",
      "  6.99930880e-18  1.85143007e-17  2.99164005e-17 -8.12822958e-18\n",
      "  2.41589046e-17 -2.03205739e-18  1.35470493e-17 -1.46759701e-17\n",
      " -3.83833063e-17  1.46759701e-17 -6.32195634e-18 -2.86745877e-17\n",
      " -1.94174373e-17  2.15623868e-17  1.35470493e-18 -1.06118553e-17\n",
      "  4.71888884e-17 -9.03136620e-19  1.21923444e-17 -7.72181810e-17\n",
      "  3.54481123e-17  0.00000000e+00 -3.88348747e-17 -1.03860711e-17\n",
      " -1.98690056e-17 -4.51568310e-19  1.89376460e-17 -5.14787873e-17\n",
      "  3.16097817e-18 -6.77352465e-19  1.27568048e-17  3.61254648e-18\n",
      "  5.08014349e-18  1.96432215e-17  2.48362570e-17  3.16097817e-17\n",
      " -1.35470493e-18 -2.82230194e-17  1.28696968e-17  1.58048908e-18\n",
      " -2.07721423e-17 -4.51568310e-18 -5.41881972e-18 -3.68028173e-17\n",
      "  6.77352465e-18  4.28989894e-17 -2.93519401e-18  1.21923444e-17\n",
      " -1.35470493e-18 -2.70940986e-18 -4.51568310e-19  1.34906033e-17\n",
      " -1.91916532e-17  1.58048908e-17  6.99930880e-18  8.48948423e-17\n",
      "  2.25784155e-17  2.66425303e-17 -9.93450282e-18  2.70940986e-18\n",
      "  3.09324292e-17 -1.53533225e-17 -6.09617218e-18 -3.34160549e-17\n",
      "  1.73853799e-17 -5.30592764e-18  7.90244542e-18  2.21268472e-17\n",
      "  4.96725141e-18  5.19303556e-18 -1.44501859e-17 -1.00473949e-17\n",
      "  1.28696968e-17 -1.62564592e-17  6.32195634e-18  2.19010630e-17\n",
      " -5.08014349e-17  4.87693775e-17  1.67080275e-17  2.43846887e-17\n",
      "  3.40087383e-17 -2.48362570e-17  3.70286014e-17 -3.95122271e-17\n",
      "  2.66425303e-17 -9.37004243e-18 -3.61254648e-18  3.86090905e-17\n",
      "  2.52878254e-17  1.28696968e-17 -4.06411479e-18  3.38676232e-17\n",
      " -1.32083731e-17 -5.63895927e-17  2.89003718e-17  7.22509296e-18\n",
      "  2.07721423e-17 -1.80627324e-18  1.25310206e-17 -1.64822433e-17\n",
      " -1.12892077e-19 -1.49017542e-17  3.43191916e-17 -1.67080275e-17\n",
      "  2.50620412e-17 -9.48293451e-18  2.66425303e-17  3.11582134e-17\n",
      "  2.25784155e-18 -5.59944704e-17  5.75749595e-18 -1.71595958e-17\n",
      " -1.08376394e-17 -1.35470493e-18 -4.51568310e-19  4.51568310e-18\n",
      "  4.80920250e-17 -1.24181285e-17 -2.75456669e-17  5.14787873e-17\n",
      " -1.30954810e-17 -8.57979789e-18  2.76585590e-17 -5.87038803e-18\n",
      " -9.03136620e-18 -5.96070169e-17  4.33505578e-17  5.59944704e-17\n",
      " -1.19665602e-17  1.08376394e-17 -3.31902708e-17  3.00292926e-17\n",
      "  3.87219826e-17  3.63512490e-17  6.77352465e-19  2.95777243e-17\n",
      "  6.09617218e-18 -5.08014349e-18 -1.12892077e-17 -1.62564592e-17\n",
      " -2.25784155e-19 -3.90606588e-17 -1.49017542e-17  9.64098342e-17\n",
      " -9.61840500e-17 -1.30954810e-17 -1.94174373e-17  6.23164268e-17\n",
      " -4.28989894e-17 -4.96725141e-17 -8.57979789e-18  2.03205739e-18\n",
      "  6.93157356e-17 -6.23164268e-17  6.18648585e-17 -8.46690581e-18\n",
      "  8.39917057e-17 -5.02369745e-17  4.42536944e-17 -1.80627324e-17\n",
      " -3.18355659e-17  3.20613500e-17  5.78007437e-17  5.87038803e-17\n",
      "  9.88934599e-17 -1.35922061e-16 -6.25422109e-17 -2.43846887e-17\n",
      " -1.80627324e-17 -6.99930880e-17 -7.76697493e-17 -2.70940986e-18\n",
      " -4.47052627e-17  1.33664220e-16 -5.58815784e-17  8.89589571e-17\n",
      " -3.47707599e-17 -7.54119078e-17  1.28245400e-16 -2.70940986e-18\n",
      " -9.93450282e-18  1.39986176e-17  1.26439127e-17  6.70578940e-17\n",
      " -2.55136095e-17  1.03860711e-17 -7.13477930e-17  1.39986176e-17\n",
      " -3.34160549e-17  8.48948423e-17 -1.01602870e-17]\n",
      "[4.48519357 4.41838958 4.18158315 4.09125445 3.99099336 3.97380104\n",
      " 3.85489103 3.78448904 3.6912725  3.64392948 3.54884075 3.5322814\n",
      " 3.41072166 3.39338727 3.36351017 3.32734531 3.23154396 3.2200488\n",
      " 3.18236276 3.13143038 3.1007145  3.05349505 3.04222778 3.00830199\n",
      " 2.97123721 2.9330194  2.91267019 2.90166538 2.83656196 2.82159304\n",
      " 2.79196756 2.78923337 2.72473447 2.69616467 2.68157798 2.66539538\n",
      " 2.65258509 2.6157285  2.61363079 2.58870367 2.55394483 2.53007901\n",
      " 2.52731827 2.49940428 2.48449626 2.47674391 2.46634403 2.45533\n",
      " 2.43626033 2.43465866 2.42149742 2.41721227 2.39640018 2.36657765\n",
      " 2.34649459 2.3331394  2.33025792 2.31987668 2.29100488 2.28004805\n",
      " 2.27041122 2.2565049  2.23899192 2.23002193 2.20886607 2.20126558\n",
      " 2.1845985  2.18088626 2.16683535 2.1545915  2.13969786 2.10920994\n",
      " 2.09994573 2.08681564 2.08480042 2.06666355 2.05075275 2.03109446\n",
      " 2.0187026  1.995593   1.98758224 1.97643931 1.96291404 1.95999975\n",
      " 1.94672726 1.94044021 1.93068455 1.92016731 1.90839696 1.90442942\n",
      " 1.89726542 1.87752548 1.87091123 1.86015134 1.8501815  1.84070903\n",
      " 1.83643609 1.82277348 1.81065623 1.80611036 1.80385519 1.79442852\n",
      " 1.79006336 1.78145351 1.77809809 1.76453211 1.74513017 1.73510283\n",
      " 1.71941268 1.7109918  1.70215239 1.69850772 1.68467724 1.68235849\n",
      " 1.66997016 1.65856425 1.65362671 1.6485385  1.64154875 1.63186863\n",
      " 1.62680302 1.60606641 1.59720972 1.57270449 1.56974775 1.56429413\n",
      " 1.55023752 1.54872144 1.53088078 1.52399178 1.5224315  1.51534573\n",
      " 1.49721301 1.48994205 1.48231    1.46889539 1.45613616 1.44613813\n",
      " 1.43649645 1.42970747 1.42419046 1.41274745 1.40585178 1.40306733\n",
      " 1.39456985 1.39007826 1.38006435 1.36631832 1.36006578 1.34637879\n",
      " 1.34187312 1.33187797 1.32281065 1.31393307 1.30795489 1.29751416\n",
      " 1.29263235 1.27927542 1.27585994 1.27421441 1.25344997 1.2473545\n",
      " 1.24562016 1.23734531 1.228484   1.2241056  1.2141187  1.20751628\n",
      " 1.19963795 1.1975474  1.19218084 1.18805355 1.18117706 1.17522295\n",
      " 1.16330533 1.16071036 1.15280557 1.14882409 1.14315569 1.13956974\n",
      " 1.133556   1.1317138  1.12532962 1.11504033 1.11065235 1.10804173\n",
      " 1.10309373 1.09755396 1.0947275  1.08340728 1.07853218 1.07215439\n",
      " 1.06797537 1.06119732 1.05755949 1.05366679 1.0471134  1.04112992\n",
      " 1.0349713  1.03062452 1.02808987 1.02803112 1.02139656 1.01877079\n",
      " 1.01814466 1.00985694 1.00834309 1.00661137 1.00582811 1.00493025\n",
      " 1.00158841 1.00078015 1.00055316 0.99856809 0.99135898 0.99057127\n",
      " 0.98773778 0.98026402 0.97752554 0.97498337 0.97287135 0.97000929\n",
      " 0.96799902 0.96604732 0.96005425 0.95911687 0.95481658 0.95407061\n",
      " 0.95221214 0.94158075 0.94066092 0.93929429 0.93716346 0.93333238\n",
      " 0.93242082 0.92832155 0.92411195 0.92134664 0.91566706 0.91425334\n",
      " 0.91116744 0.91016899 0.90642238 0.90440185 0.89867959 0.89375641\n",
      " 0.89159892 0.88488958 0.87703106 0.87417411 0.8704697  0.86788287\n",
      " 0.86575742 0.86309634 0.85861999 0.85810151 0.85611296 0.84611125\n",
      " 0.84348793 0.84070023 0.83190594 0.82688556 0.82579699 0.82136906\n",
      " 0.81535999 0.8084336  0.80699419 0.80594819 0.80038208 0.79187581\n",
      " 0.78950923 0.78693753 0.7839544  0.7763843  0.77176352 0.76876016\n",
      " 0.76460727 0.76332453 0.7601195  0.75864556 0.75218668 0.7430787\n",
      " 0.74021978 0.73700043 0.73220971 0.7247246  0.7184589  0.71510574\n",
      " 0.71276879 0.70945602 0.706674   0.69841224 0.69696326 0.69545415\n",
      " 0.69275773 0.6914143  0.68858193 0.68549858 0.68291906 0.67599599\n",
      " 0.67405785 0.6721546  0.66639492 0.6594172  0.65723223 0.64771146\n",
      " 0.64194213 0.63785445 0.63343055 0.62729451 0.62563395 0.62314431\n",
      " 0.61577718 0.61257954 0.60765139 0.60164422 0.59724483 0.59489836\n",
      " 0.59272769 0.58691498 0.58614321 0.58171388 0.57873992 0.57574242\n",
      " 0.56478894 0.56327011 0.5598305  0.55662043 0.55303066 0.55228135\n",
      " 0.54697376 0.54304992 0.54211855 0.54085664 0.5403627  0.53630251\n",
      " 0.53237853 0.5284367  0.5263738  0.52423167 0.52245315 0.51571207\n",
      " 0.51146287 0.51021759 0.50927206 0.50870492 0.50238606 0.5003056\n",
      " 0.49561968 0.49244813 0.48878688 0.48711653 0.48417338 0.48054995\n",
      " 0.47923815 0.4768327  0.47489703 0.47346099 0.47164918 0.46750585\n",
      " 0.46302378 0.46053596 0.45571253 0.45395503 0.4522324  0.45034166\n",
      " 0.43979245 0.43634641 0.43449446 0.42938313 0.42610271 0.42492323\n",
      " 0.42205824 0.41881406 0.41724354 0.41178765 0.4083318  0.40686231\n",
      " 0.40284381 0.39971199 0.39659871 0.39354126 0.39133026 0.39101916\n",
      " 0.390149   0.38908103 0.38520333 0.38241956 0.37862867 0.37600567\n",
      " 0.37382726 0.37342053 0.36848171 0.368096   0.36323609 0.36196938\n",
      " 0.35600351 0.3526863  0.3458908  0.34507879 0.34380204 0.34307796\n",
      " 0.3417318  0.33865088 0.33677716 0.33470022 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.        ]\n",
      "[[ 0.65823761 -1.15369358 -2.19785686 ... -0.3664687  -0.09351828\n",
      "  -0.78657864]\n",
      " [-0.47275403  0.09177634 -1.09118616 ...  2.728746   -0.09351828\n",
      "  -0.97314237]\n",
      " [-0.05999018 -0.00738919  0.72119362 ... -0.3664687  -0.09351828\n",
      "  -0.67814843]\n",
      " ...\n",
      " [-0.6284407   0.57387826  0.03121245 ...  2.728746   -0.09351828\n",
      "   1.33897231]\n",
      " [-1.06499891  1.08624208  0.3772333  ... -0.3664687  -0.09351828\n",
      "  -0.27738188]\n",
      " [-0.36516214 -0.12348675 -1.30767466 ...  2.728746   -0.09351828\n",
      "  -0.52134985]]\n",
      "[[ 0.20762102 -0.74389718  1.4117418  ...  2.728746   -0.09351828\n",
      "   0.81914515]\n",
      " [ 0.24165612 -0.90455956  1.44208854 ... -0.3664687  -0.09351828\n",
      "  -0.74033634]\n",
      " [ 0.21360657 -0.99094413 -1.93630162 ...  2.728746   -0.09351828\n",
      "  -0.41876637]\n",
      " ...\n",
      " [-0.61323949  0.30520345 -0.65943869 ... -0.3664687  -0.09351828\n",
      "   0.58208695]\n",
      " [-1.17027926  1.06416577 -0.29396551 ... -0.3664687  -0.09351828\n",
      "   1.08384242]\n",
      " [ 0.16012782 -1.09045957  1.40713148 ... -0.3664687  -0.09351828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -0.72439072]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31262, 455)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#z-score 标准化\n",
    "scaler=StandardScaler().fit(dg_train)#标准化的mean var\n",
    "print(scaler)\n",
    "print(scaler.mean_)\n",
    "print(scaler.var_)\n",
    "dg_scaled_train=scaler.transform(dg_train)#标准化结果向量\n",
    "print(dg_scaled_train)\n",
    "np.shape(dg_scaled_train)\n",
    "\n",
    "dg_scaled_test=scaler.transform(dg_test)\n",
    "print(dg_scaled_test)\n",
    "np.shape(dg_scaled_test)#相同标准 标准化测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_scaled_train=dg_train.values\n",
    "dg_scaled_test=dg_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.39403375, -2.42505996, -4.49437984, ..., -0.3664687 ,\n",
       "        -0.09351828, -0.78657864],\n",
       "       [-1.00121151,  0.19291356, -2.23135782, ...,  2.728746  ,\n",
       "        -0.09351828, -0.97314237],\n",
       "       [-0.12704885, -0.01553204,  1.47476304, ..., -0.3664687 ,\n",
       "        -0.09351828, -0.67814843],\n",
       "       ...,\n",
       "       [-1.33092903,  1.20629013,  0.0638261 , ...,  2.728746  ,\n",
       "        -0.09351828,  1.33897231],\n",
       "       [-2.25548404,  2.28327712,  0.77140135, ..., -0.3664687 ,\n",
       "        -0.09351828, -0.27738188],\n",
       "       [-0.77335044, -0.25956873, -2.67405341, ...,  2.728746  ,\n",
       "        -0.09351828, -0.52134985]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_scaled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下开始特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = clf_fs.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "\n",
    "\n",
    "fi_threshold = 18    \n",
    "important_idx = np.where(feature_importance > fi_threshold)[0]\n",
    "important_features = features_list[important_idx]\n",
    "\n",
    "\n",
    "    print(\"\\n\", important_features.shape[0], \"Important features(>\", \\\n",
    "          fi_threshold, \"% of max importance)...\\n\")\n",
    "            #important_features\n",
    "    sorted_idx = np.argsort(feature_importance[important_idx])[::-1]\n",
    "    #get the figure about important features\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.barh(pos, feature_importance[important_idx][sorted_idx[::-1]], \\\n",
    "            color='r',align='center')\n",
    "    plt.yticks(pos, important_features[sorted_idx[::-1]])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.65308893e-03, 6.87972795e-04, 7.98705089e-05, 2.27842639e-04,\n",
       "       1.46403215e-02, 1.17930950e-03, 4.19014453e-04, 1.18219355e-02,\n",
       "       2.03191060e-03, 2.37593988e-04, 2.09931462e-03, 3.76941547e-04,\n",
       "       0.00000000e+00, 1.19562443e-04, 6.24221393e-05, 2.69610041e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.40881395e-02, 0.00000000e+00, 3.17013300e-03, 1.97824946e-05,\n",
       "       4.99777447e-04, 0.00000000e+00, 6.09020318e-05, 3.99213480e-04,\n",
       "       2.99983727e-04, 2.96118870e-04, 9.43223185e-04, 0.00000000e+00,\n",
       "       1.19067177e-03, 0.00000000e+00, 4.15141401e-03, 0.00000000e+00,\n",
       "       3.14848541e-04, 0.00000000e+00, 0.00000000e+00, 1.20072690e-03,\n",
       "       5.05467446e-05, 1.01156895e-03, 0.00000000e+00, 8.10979885e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.86291664e-04, 6.89438166e-05,\n",
       "       1.87533182e-04, 0.00000000e+00, 3.02425856e-04, 1.09734144e-04,\n",
       "       1.82440130e-04, 9.39934112e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.33279140e-05, 3.20569763e-04,\n",
       "       2.21317058e-04, 0.00000000e+00, 2.84014205e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 8.71794255e-04, 2.10783723e-03, 3.43280685e-04,\n",
       "       3.63799680e-04, 5.54615973e-03, 1.75884389e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.44981619e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.98346132e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.41093162e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.38585358e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       5.82860342e-06, 0.00000000e+00, 0.00000000e+00, 8.05926586e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.73709113e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.39322044e-03, 0.00000000e+00, 3.10719949e-05,\n",
       "       1.31850143e-03, 1.96050759e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.04115216e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.41302114e-04, 3.12071370e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.33076429e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.87333919e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.47161392e-04, 2.83023491e-04, 0.00000000e+00,\n",
       "       9.22988884e-05, 9.58714055e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.30009258e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.54492320e-04, 0.00000000e+00, 0.00000000e+00, 1.66995523e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.49339357e-04, 9.83008669e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.36681045e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.29621099e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.22621355e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 5.38829123e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.35062611e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.17415908e-04, 2.81218182e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.75346446e-04,\n",
       "       3.94705989e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.83855429e-04, 6.99218275e-05,\n",
       "       2.18163378e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.21622760e-04, 7.35386122e-05, 5.40053254e-06, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.76831101e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.16977561e-04, 9.33883081e-05,\n",
       "       0.00000000e+00, 4.45821640e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 8.80769410e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.53489565e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.39609557e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.21704618e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.46595219e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.62628129e-05, 0.00000000e+00, 8.02783457e-04,\n",
       "       0.00000000e+00, 1.72811619e-04, 0.00000000e+00, 5.19546353e-03,\n",
       "       6.05696316e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.94650566e-05,\n",
       "       0.00000000e+00, 8.84854390e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.65864886e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 6.62175591e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.01017926e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.58954083e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.26530837e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.34849590e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.25467699e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 5.26737427e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.01156816e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.14999803e-06, 7.85018829e-04,\n",
       "       0.00000000e+00, 1.00239107e-04, 0.00000000e+00, 2.87803788e-03,\n",
       "       0.00000000e+00, 4.47625786e-04, 0.00000000e+00, 3.53439852e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.05435986e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.34118199e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.48204454e-04, 0.00000000e+00, 8.11635683e-05, 1.96601645e-03,\n",
       "       0.00000000e+00, 1.14908799e-02, 5.63848721e-02, 1.03943623e-03,\n",
       "       2.33191705e-04, 0.00000000e+00, 7.59435289e-04, 0.00000000e+00,\n",
       "       3.54633971e-03, 1.30796180e-03, 1.44485080e-01, 4.39188074e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.60627415e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.66554650e-03, 7.52528296e-02, 3.37343196e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 9.18565264e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.97262625e-04, 0.00000000e+00,\n",
       "       2.04544852e-05, 1.03861261e-02, 5.18283473e-01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_fs_1 = ensemble.GradientBoostingRegressor(random_state=120)\n",
    "clf_fs_1 =clf_fs_1.fit(dg_scaled_train, score_train)\n",
    "clf_fs_1.feature_importances_  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.56973137e-03, 5.23677149e-03, 1.95089858e-03, 1.49786924e-03,\n",
       "       8.61559335e-03, 2.09932864e-03, 1.05629663e-03, 3.95588398e-03,\n",
       "       3.61580852e-03, 6.36115711e-04, 3.89773453e-04, 1.21090859e-03,\n",
       "       4.19374946e-04, 5.26169368e-04, 9.09901180e-04, 7.63527595e-04,\n",
       "       9.81813608e-04, 2.36605302e-04, 5.73178138e-04, 8.07095642e-04,\n",
       "       4.24256597e-03, 4.84334951e-03, 5.94364135e-03, 3.62061574e-04,\n",
       "       8.89683729e-04, 1.41843082e-03, 3.47318002e-04, 1.31193146e-03,\n",
       "       5.68636319e-04, 5.91936549e-04, 2.96343579e-04, 3.76104785e-04,\n",
       "       3.28966964e-04, 3.02123450e-04, 7.41348409e-03, 2.76394542e-04,\n",
       "       2.97538574e-03, 5.09704365e-04, 1.40631157e-03, 2.06312184e-03,\n",
       "       6.84847074e-04, 5.99204103e-04, 5.59722408e-04, 2.64486154e-04,\n",
       "       4.81862824e-04, 5.90611236e-04, 1.88978844e-04, 6.91439949e-04,\n",
       "       2.91190863e-04, 1.28102856e-03, 4.28018909e-04, 2.96990561e-04,\n",
       "       4.93717525e-04, 5.99766507e-04, 6.17614530e-04, 8.72921638e-04,\n",
       "       2.55973939e-04, 1.16138378e-03, 4.86800100e-04, 2.00891702e-04,\n",
       "       4.47748842e-04, 2.01742757e-04, 4.08387650e-03, 3.82360784e-04,\n",
       "       2.05260050e-04, 1.45991659e-03, 2.85494844e-04, 1.46666086e-04,\n",
       "       2.46957058e-04, 1.25726977e-04, 7.51446840e-04, 3.24525934e-04,\n",
       "       7.57782031e-04, 5.96165427e-04, 1.04805074e-03, 3.81386838e-04,\n",
       "       7.11784249e-04, 6.50748059e-04, 3.37423453e-04, 5.43128533e-04,\n",
       "       3.21317004e-04, 3.67321601e-03, 1.57980252e-04, 3.16392864e-04,\n",
       "       8.22246599e-04, 1.90561297e-04, 6.61600555e-03, 1.08823947e-04,\n",
       "       2.78969424e-04, 2.44194554e-04, 2.02520351e-04, 1.09991987e-04,\n",
       "       5.15092851e-04, 2.75390049e-04, 7.20182950e-04, 6.94639591e-04,\n",
       "       1.64632152e-04, 1.37889818e-04, 1.13128178e-04, 9.93982804e-05,\n",
       "       2.47734342e-03, 2.99661618e-04, 1.73841633e-04, 1.86987142e-04,\n",
       "       1.88413667e-04, 5.26137916e-04, 3.45958493e-04, 1.01701629e-03,\n",
       "       2.89378858e-03, 1.97266175e-04, 1.16610871e-04, 5.87104467e-04,\n",
       "       3.34429140e-04, 2.87248091e-04, 8.27704931e-05, 1.83222138e-04,\n",
       "       2.14707714e-04, 1.38651791e-04, 1.04616054e-04, 1.79268786e-04,\n",
       "       2.19420949e-04, 2.70670293e-04, 2.99270401e-04, 1.44122239e-04,\n",
       "       2.99782496e-04, 2.42254881e-04, 1.37101237e-04, 3.80109120e-04,\n",
       "       3.81405348e-04, 1.55224058e-04, 1.82488141e-04, 3.51929947e-04,\n",
       "       2.51633403e-04, 1.99227925e-04, 1.07936648e-03, 1.85670757e-04,\n",
       "       1.72584275e-04, 2.24050386e-04, 1.78483360e-04, 2.05130083e-04,\n",
       "       1.31497390e-04, 1.40064782e-04, 4.95084380e-04, 1.88240848e-04,\n",
       "       1.01895839e-04, 1.40559068e-04, 9.94839778e-05, 1.54615952e-04,\n",
       "       3.96457680e-04, 1.49001386e-04, 5.13010299e-04, 1.58507550e-04,\n",
       "       3.38074945e-04, 1.56320804e-04, 7.22376288e-04, 7.41086631e-04,\n",
       "       5.77961819e-04, 7.13473078e-04, 2.05659514e-04, 1.69961330e-04,\n",
       "       1.46779204e-04, 1.10250626e-04, 6.38748622e-04, 2.20079183e-04,\n",
       "       7.59838614e-05, 1.10295097e-04, 2.41176461e-04, 1.04728643e-03,\n",
       "       7.19411072e-05, 1.24134171e-04, 1.64485828e-04, 1.18366241e-04,\n",
       "       2.99986698e-04, 2.38571699e-04, 3.59948031e-04, 1.32866783e-04,\n",
       "       8.47892888e-04, 9.30311374e-05, 1.43354865e-04, 1.66881383e-04,\n",
       "       1.10184607e-04, 1.04966584e-04, 1.58852303e-04, 1.20029673e-04,\n",
       "       1.39392779e-04, 3.07240815e-04, 3.16172007e-04, 3.15785942e-04,\n",
       "       2.46011628e-04, 2.81866049e-04, 8.24769440e-04, 2.05337944e-04,\n",
       "       8.13395432e-05, 1.83952015e-04, 2.54917756e-04, 1.37951260e-04,\n",
       "       2.72628161e-04, 1.93709486e-04, 2.08364998e-04, 2.62088802e-03,\n",
       "       2.78549398e-04, 5.65768812e-04, 6.59021692e-04, 1.91470658e-04,\n",
       "       2.63456980e-04, 1.11723778e-04, 1.81517291e-04, 1.61267735e-04,\n",
       "       6.45616848e-05, 6.26593178e-04, 1.00869923e-04, 2.73318507e-04,\n",
       "       1.01059961e-04, 1.93293675e-04, 2.34447140e-04, 8.14411322e-04,\n",
       "       1.96675162e-04, 1.17787883e-04, 1.37313575e-04, 2.70245028e-03,\n",
       "       8.07035982e-04, 1.33623621e-04, 1.34815516e-04, 3.64631145e-04,\n",
       "       3.82575277e-04, 2.67464391e-04, 7.89461345e-05, 1.70295153e-04,\n",
       "       1.91595915e-04, 7.60232495e-05, 9.93305834e-05, 2.74526877e-04,\n",
       "       4.05543967e-04, 2.93105348e-04, 1.84042337e-04, 2.32445883e-04,\n",
       "       1.47751727e-04, 1.21297260e-04, 2.08039861e-04, 1.48613860e-04,\n",
       "       2.21209230e-04, 9.09751049e-05, 9.98986947e-05, 1.84837101e-04,\n",
       "       1.20775515e-04, 4.82843427e-04, 1.63300254e-04, 2.57393274e-04,\n",
       "       2.94915400e-04, 1.29286895e-04, 9.20449591e-05, 1.02225939e-04,\n",
       "       1.34344514e-04, 1.53459250e-04, 2.15256550e-04, 6.32587525e-05,\n",
       "       1.58379368e-04, 2.44946761e-04, 2.80195816e-04, 1.59848305e-04,\n",
       "       1.92768379e-04, 1.25364708e-04, 1.52633540e-04, 1.61487615e-04,\n",
       "       1.05725082e-03, 8.82733003e-05, 2.56076599e-04, 2.19892384e-04,\n",
       "       1.07165805e-04, 1.50998048e-04, 1.00931363e-04, 4.03237067e-04,\n",
       "       2.40221968e-04, 2.08705517e-04, 1.66295330e-03, 5.14503918e-04,\n",
       "       1.69755782e-04, 2.36789179e-04, 3.17923399e-04, 1.69513237e-04,\n",
       "       3.89611735e-04, 1.26391749e-04, 1.23766184e-04, 4.98207138e-04,\n",
       "       2.29559337e-04, 1.77735554e-04, 1.76580420e-04, 1.79018438e-04,\n",
       "       3.26337997e-04, 9.15831603e-05, 5.44152555e-04, 2.41587107e-04,\n",
       "       9.55461481e-05, 7.27171868e-04, 3.01718672e-04, 2.21459128e-04,\n",
       "       8.12495054e-04, 1.01022438e-04, 1.79213519e-04, 1.33301769e-04,\n",
       "       1.48101506e-04, 2.97487835e-04, 2.25324751e-04, 7.95781987e-04,\n",
       "       1.07918295e-03, 3.56300537e-04, 1.40630702e-04, 4.66386465e-04,\n",
       "       1.27936478e-04, 1.47373748e-04, 1.38521489e-03, 1.62075275e-03,\n",
       "       1.45296543e-04, 4.83161488e-04, 2.91296357e-04, 7.28309624e-03,\n",
       "       1.38310222e-04, 2.05536029e-04, 8.93241951e-05, 1.60363853e-04,\n",
       "       6.46641835e-04, 3.17504758e-04, 1.23833122e-04, 1.20538719e-03,\n",
       "       1.62855146e-04, 4.03486866e-04, 1.63066338e-03, 1.73281126e-04,\n",
       "       1.51486734e-04, 1.77896442e-04, 1.78493614e-04, 1.65444944e-04,\n",
       "       9.73185096e-05, 1.87435715e-04, 8.87501238e-04, 4.94224871e-04,\n",
       "       2.96256351e-04, 2.05660545e-04, 1.29152828e-04, 4.23691022e-04,\n",
       "       2.03209515e-04, 1.75986470e-04, 1.47673423e-04, 1.39130601e-04,\n",
       "       1.42333007e-04, 1.98570974e-04, 6.24839346e-04, 2.09845985e-04,\n",
       "       2.13134088e-04, 7.65411666e-05, 9.60578906e-05, 2.47037080e-03,\n",
       "       1.67817251e-04, 1.05989239e-04, 6.35803431e-05, 8.89097147e-05,\n",
       "       3.17623409e-04, 1.53316929e-04, 1.62983271e-04, 1.77702003e-04,\n",
       "       9.54308182e-05, 1.44377462e-04, 1.22442790e-03, 1.85842836e-04,\n",
       "       2.54342677e-04, 1.27223479e-04, 1.30402481e-04, 1.75640625e-04,\n",
       "       1.75808375e-04, 2.36235732e-04, 1.91701363e-04, 3.15031150e-04,\n",
       "       1.58805828e-04, 9.89270066e-05, 1.42106518e-04, 4.92313738e-04,\n",
       "       2.25038772e-04, 2.32290243e-04, 1.07426722e-04, 6.01971108e-04,\n",
       "       1.16038116e-04, 1.87181217e-04, 1.65094039e-04, 4.48063609e-04,\n",
       "       3.16092114e-04, 1.40855501e-04, 7.12832593e-04, 1.47305748e-04,\n",
       "       1.70520834e-04, 1.73483465e-04, 1.60758215e-04, 6.33175168e-04,\n",
       "       2.10539571e-04, 6.61507599e-04, 2.70826345e-04, 2.45203557e-03,\n",
       "       5.96143016e-04, 8.21368260e-04, 1.25585747e-04, 1.21290533e-04,\n",
       "       9.82751909e-05, 2.36494655e-04, 2.87787361e-04, 1.88126693e-04,\n",
       "       3.55792521e-04, 2.59395379e-04, 8.00161034e-06, 2.02119365e-05,\n",
       "       1.17156465e-05, 8.32855529e-07, 2.27462822e-06, 4.25079850e-05,\n",
       "       3.77979012e-05, 1.27261818e-06, 8.13177226e-07, 6.49956715e-05,\n",
       "       4.52339294e-06, 1.08919550e-05, 1.38342282e-05, 1.98780872e-04,\n",
       "       1.55558209e-05, 5.47025599e-03, 3.15375737e-02, 1.55907535e-04,\n",
       "       9.17894444e-05, 2.16968549e-07, 6.11830665e-05, 5.59303513e-06,\n",
       "       6.27056215e-04, 3.95212636e-04, 3.17608786e-01, 3.08966211e-02,\n",
       "       5.37207605e-05, 2.50120607e-04, 6.29648009e-05, 2.76355010e-04,\n",
       "       5.68440140e-05, 7.20580134e-05, 1.44021585e-04, 9.91955535e-05,\n",
       "       4.72957264e-03, 6.89068552e-02, 1.04544141e-03, 3.76890317e-07,\n",
       "       0.00000000e+00, 5.16915002e-08, 8.29940197e-03, 7.64440470e-05,\n",
       "       6.73000523e-07, 0.00000000e+00, 4.52411633e-04, 1.12102879e-05,\n",
       "       2.50596974e-05, 4.69396250e-03, 2.97103800e-01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_fs_2 = RandomForestRegressor(random_state=150)\n",
    "clf_fs_2 =clf_fs_2.fit(dg_scaled_train, score_train)\n",
    "clf_fs_2.feature_importances_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:14:39] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.0039762 , 0.0056835 , 0.01634524, 0.00157798, 0.02455863,\n",
       "       0.00721036, 0.00232874, 0.01604441, 0.00870163, 0.00530214,\n",
       "       0.02031631, 0.00389328, 0.        , 0.00269644, 0.0025021 ,\n",
       "       0.        , 0.        , 0.0034817 , 0.00322748, 0.00289669,\n",
       "       0.02975027, 0.00078814, 0.02763163, 0.        , 0.0038662 ,\n",
       "       0.        , 0.        , 0.00453342, 0.00311516, 0.0016042 ,\n",
       "       0.01546049, 0.        , 0.00184897, 0.        , 0.00815811,\n",
       "       0.        , 0.00178288, 0.00723859, 0.        , 0.02439605,\n",
       "       0.        , 0.00619278, 0.        , 0.00285307, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00158517, 0.        ,\n",
       "       0.0025359 , 0.00136969, 0.00140693, 0.00381462, 0.        ,\n",
       "       0.00091361, 0.        , 0.        , 0.00190803, 0.00487874,\n",
       "       0.        , 0.        , 0.02906667, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00069306, 0.        , 0.        ,\n",
       "       0.00725177, 0.00303634, 0.00244387, 0.02058104, 0.0024389 ,\n",
       "       0.        , 0.00042853, 0.00160238, 0.00344099, 0.        ,\n",
       "       0.        , 0.00090145, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00281523, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00399836, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04196411, 0.        , 0.00492769,\n",
       "       0.00916492, 0.00128867, 0.        , 0.        , 0.01382084,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00325938, 0.00027703, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.0015752 , 0.001092  , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00172141, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00269427, 0.00210998, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00142072, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00243958, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00136634, 0.        ,\n",
       "       0.        , 0.00328754, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00194136, 0.00108957,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.002319  ,\n",
       "       0.00282865, 0.        , 0.        , 0.00031834, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00094091, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00226716,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00085733,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00148882,\n",
       "       0.00534302, 0.        , 0.00874753, 0.        , 0.        ,\n",
       "       0.        , 0.00257059, 0.        , 0.        , 0.00302636,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00132905, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00821117, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00200627, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.001459  ,\n",
       "       0.00183898, 0.        , 0.        , 0.        , 0.00164442,\n",
       "       0.        , 0.        , 0.0007889 , 0.        , 0.00021307,\n",
       "       0.        , 0.        , 0.        , 0.00250632, 0.        ,\n",
       "       0.00127838, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00851088, 0.        , 0.        , 0.00145913, 0.00158249,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00888167, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00406246, 0.        ,\n",
       "       0.        , 0.        , 0.00232162, 0.01177018, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00366028, 0.        , 0.        , 0.        ,\n",
       "       0.00085898, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00394278, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00689709, 0.        , 0.        , 0.        ,\n",
       "       0.00700045, 0.        , 0.00385928, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00901591, 0.        , 0.        ,\n",
       "       0.0079315 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00104592, 0.        , 0.00165774, 0.00570955,\n",
       "       0.        , 0.02198488, 0.06131452, 0.00383453, 0.00154354,\n",
       "       0.        , 0.00346967, 0.        , 0.01312809, 0.00501252,\n",
       "       0.11614309, 0.0132383 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.005178  , 0.04207701, 0.00099967, 0.        , 0.        ,\n",
       "       0.        , 0.02883762, 0.        , 0.        , 0.        ,\n",
       "       0.00089937, 0.        , 0.00039578, 0.03289544, 0.04438214],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_fs_3 = xgb.XGBRegressor(random_state=151)\n",
    "clf_fs_3 =clf_fs_3.fit(dg_scaled_train, score_train)\n",
    "clf_fs_3.feature_importances_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_importance_1=clf_fs_1.feature_importances_\n",
    "#feature_importance_2=clf_fs_2.feature_importances_\n",
    "feature_importance_3=clf_fs_3.feature_importances_\n",
    "feature_importance=np.add(feature_importance_1,feature_importance_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.14999803e-06, 5.40053254e-06, 5.82860342e-06,\n",
       "       1.60627415e-05, 1.97824946e-05, 2.38585358e-05, 3.12071370e-05,\n",
       "       3.29621099e-05, 5.05467446e-05, 5.26737427e-05, 6.05696316e-05,\n",
       "       6.09020318e-05, 6.58954083e-05, 6.62175591e-05, 6.89438166e-05,\n",
       "       6.99218275e-05, 7.35386122e-05, 9.83008669e-05, 1.00239107e-04,\n",
       "       1.21622760e-04, 1.30009258e-04, 1.33076429e-04, 1.41093162e-04,\n",
       "       1.49339357e-04, 1.65864886e-04, 1.86291664e-04, 2.01017926e-04,\n",
       "       2.01156816e-04, 2.18163378e-04, 2.21317058e-04, 2.25467699e-04,\n",
       "       2.26530837e-04, 2.29334226e-04, 2.47161392e-04, 2.69610041e-04,\n",
       "       2.77028361e-04, 2.83023491e-04, 3.18337552e-04, 3.53439852e-04,\n",
       "       3.54492320e-04, 4.16237126e-04, 4.28534113e-04, 6.93063252e-04,\n",
       "       7.88139470e-04, 7.88898906e-04, 8.02783457e-04, 8.57325387e-04,\n",
       "       8.58978601e-04, 8.71794255e-04, 8.84854390e-04, 9.13607539e-04,\n",
       "       1.12476847e-03, 1.14643412e-03, 1.19663059e-03, 1.32905063e-03,\n",
       "       1.33330065e-03, 1.33701321e-03, 1.35062611e-03, 1.37078666e-03,\n",
       "       1.45899667e-03, 1.47942513e-03, 1.48471692e-03, 1.48896342e-03,\n",
       "       1.49412662e-03, 1.52859689e-03, 1.57519639e-03, 1.58249238e-03,\n",
       "       1.58771798e-03, 1.58937401e-03, 1.60238263e-03, 1.60580100e-03,\n",
       "       1.73890192e-03, 1.77269845e-03, 1.77673373e-03, 1.80581798e-03,\n",
       "       1.86115001e-03, 1.90031712e-03, 1.90874086e-03, 1.95136084e-03,\n",
       "       1.98346132e-03, 1.99101542e-03, 2.05877877e-03, 2.09773243e-03,\n",
       "       2.14587606e-03, 2.20584906e-03, 2.44398666e-03, 2.52324920e-03,\n",
       "       2.55646750e-03, 2.56451826e-03, 2.61478096e-03, 2.65866997e-03,\n",
       "       2.67913268e-03, 2.74774953e-03, 2.78656969e-03, 2.80767393e-03,\n",
       "       2.81523331e-03, 2.81600044e-03, 2.83833038e-03, 2.89434501e-03,\n",
       "       2.89669354e-03, 3.03964475e-03, 3.22335174e-03, 3.22747766e-03,\n",
       "       3.25937709e-03, 3.37962301e-03, 3.37984812e-03, 3.41514202e-03,\n",
       "       3.44098988e-03, 3.48169636e-03, 3.66028468e-03, 3.66404701e-03,\n",
       "       3.73709113e-03, 3.82636591e-03, 3.94278439e-03, 4.06246493e-03,\n",
       "       4.22910571e-03, 4.27022544e-03, 4.30691012e-03, 4.36597965e-03,\n",
       "       4.75455897e-03, 4.80428811e-03, 4.87396345e-03, 4.93263007e-03,\n",
       "       4.95876586e-03, 5.19930633e-03, 5.43641246e-03, 5.53973541e-03,\n",
       "       6.32047959e-03, 6.37147684e-03, 6.47384754e-03, 6.62929336e-03,\n",
       "       7.20434579e-03, 7.23859202e-03, 7.67556214e-03, 7.68210697e-03,\n",
       "       8.21117312e-03, 8.38966522e-03, 8.51087831e-03, 8.88166949e-03,\n",
       "       9.19335367e-03, 9.35960607e-03, 9.84354355e-03, 9.87848467e-03,\n",
       "       1.02726780e-02, 1.04834169e-02, 1.07335405e-02, 1.10702711e-02,\n",
       "       1.17701767e-02, 1.23095204e-02, 1.38412553e-02, 1.64037144e-02,\n",
       "       1.64251125e-02, 1.66744266e-02, 2.24156211e-02, 2.55967814e-02,\n",
       "       2.61271965e-02, 2.78663492e-02, 3.08017604e-02, 3.19068109e-02,\n",
       "       3.34757588e-02, 3.80232739e-02, 3.91989495e-02, 4.32815645e-02,\n",
       "       4.33573267e-02, 4.38384103e-02, 5.71571089e-02, 1.17329842e-01,\n",
       "       1.17699392e-01, 2.60628172e-01, 5.62665609e-01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_importance 170维\n",
    "index_fs=np.argsort(-feature_importance)\n",
    "index_fs_select=index_fs[0:170]\n",
    "X=dg_scaled_train[:,index_fs_select]\n",
    "X_predict=dg_scaled_test[:,index_fs_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[454 430 422 441 431  20 117 453   4 446 421  62  22   7  73  39  10 428\n",
      "   2  30 124  34 363 407   8 120 410 395 440  70 257 333 320   5 281 391\n",
      " 419  37  41   0 315   1 429   9 255  59 119  27 423 107  53  24 397  11\n",
      " 426 358 386 201 111  43 376  17  78  28 264  71 130  18 220  32  19 219\n",
      "  50  13 101  72 156   6 313 261  74  14 362 176 244 157 290  36 213 304\n",
      "  94  58 150  29 300   3 424  48 418 254  77  52 167 324 137 323 416 198\n",
      " 121  51 299 214 204 442 138 272 450  81 230  55 325  69 380 249 311 307\n",
      "  21  67  76 452 164 399 223 154 131  15 153 309 357 366  60 232 387 344\n",
      "  46 334 172  97 145 160 236 393 173 237 231  47 337 351  26 316 369  40\n",
      " 189 139 100  23 435 104 238 390]\n"
     ]
    }
   ],
   "source": [
    "print(index_fs_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31470, 170)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dg_scaled_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#不运行\n",
    "#基模型的特征选择\n",
    "clf_fs = ensemble.GradientBoostingRegressor(random_state=20)\n",
    "clf_fs =clf_fs.fit(dg_scaled_train, score_train)\n",
    "clf_fs.feature_importances_  \n",
    "\n",
    "model_fs = SelectFromModel(clf_fs, prefit=True)\n",
    "X = model_fs.transform(dg_scaled_train)\n",
    "\n",
    "\n",
    "#SelectFromModel(ensemble.GradientBoostingRegressor()).fit_transform(dg_scaled_train, score_train)\n",
    "\n",
    "X_predict=model_fs.transform(dg_scaled_test)\n",
    "print(dg_scaled_test)\n",
    "np.shape(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25176, 170), (25176,), (6294, 170), (6294,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#划分数据集\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, score_train, test_size=0.2, random_state=21)\n",
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_actual, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、学習（train）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7316969922445721\n",
      "0.7316969922445721\n",
      "38590.827874245486\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from sklearn import linear_model\n",
    "model_reg = linear_model.Ridge()\n",
    "model_reg.fit(X_train,y_train)\n",
    "\n",
    "print(model_reg.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_reg.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7316845191843815\n",
      "0.7316845191843815\n",
      "38591.72488268522\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "model_reg = linear_model.LinearRegression()\n",
    "model_reg.fit(X_train,y_train)\n",
    "\n",
    "print(model_reg.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_reg.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7328717036698977\n",
      "0.7328717036698977\n",
      "38506.25406309121\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "model_reg = linear_model.LassoLars()\n",
    "model_reg.fit(X_train,y_train)\n",
    "\n",
    "print(model_reg.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_reg.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7321818676008225\n",
      "0.7321818676008225\n",
      "38555.94156575715\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "model_reg = linear_model.BayesianRidge()\n",
    "model_reg.fit(X_train,y_train)\n",
    "\n",
    "print(model_reg.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_reg.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04144854451134572\n",
      "-0.04144854451134572\n",
      "76030.9178712978\n"
     ]
    }
   ],
   "source": [
    "#SVR模型实现\n",
    "model_svc=svm.SVR()\n",
    "model_svc.fit(X_train,y_train)\n",
    "\n",
    "print(model_svc.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_svc.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT 0.5903171534507402<br>\n",
    "RF 0.5714306438983159<br >\n",
    "XBG 0.5864080375808052<br>\n",
    "平均500 0.5877608712908593<br>\n",
    "平均400 0.5889615497881555<br>\n",
    "358 0.5888839063663873<br>\n",
    "370 0.5889239406579279<br>\n",
    "375 0.589024306528543<br>\n",
    "380 0.5897356830942392<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR模型调参数：C gamma\n",
    "param_g=[{\"C\": [1e0, 1e1, 1e2, 1e3],\"gamma\": np.logspace(-4, 0, 5)}]\n",
    "scores_g=['r2']\n",
    "for score in scores_g:\n",
    "    print(score)\n",
    "    clf=GridSearchCV(svm.SVR(),param_grid=param_g,cv=3,scoring=score)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds=clf.cv_results_['std_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "    \n",
    "    for (param,mean_score,std_score) in zip(params,means,stds):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score,std_score,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR模型实现\n",
    "model_svc_t=clf.best_estimator_\n",
    "model_svc_t.fit(X_train,y_train)\n",
    "\n",
    "print(model_svc_t.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_svc_t.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "\n",
    "model_svc=clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8681192598059411\n",
      "0.8681192598059411\n",
      "27055.90172620822\n"
     ]
    }
   ],
   "source": [
    "model_svcRF=RandomForestRegressor(n_estimators=200,random_state=160)\n",
    "model_svcRF.fit(X_train,y_train)\n",
    "\n",
    "print(model_svcRF.score(X_test,y_test))#0.4697676007383277\n",
    "\n",
    "y_pred = model_svcRF.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "380 0.4808578016857748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-1cd2c74c9332>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    664\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 666\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 330\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1155\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1158\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_estimators=[{\"n_estimators\":[150,200,300,400,500]}]#20,50,100,\n",
    "scores_g=['r2']\n",
    "for score in scores_g:\n",
    "    print(score)\n",
    "    clf=GridSearchCV(RandomForestRegressor(random_state=160),param_grid=N_estimators,cv=3,scoring=score)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds=clf.cv_results_['std_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "    \n",
    "    for (param,mean_score,std_score) in zip(params,means,stds):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score,std_score,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r2\n",
    "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=200,\n",
    "                      n_jobs=None, oob_score=False, random_state=160, verbose=0,\n",
    "                      warm_start=False)\n",
    "0.881(+/-0.027) for {'n_estimators': 20}\n",
    "0.888(+/-0.029) for {'n_estimators': 50}\n",
    "0.888(+/-0.029) for {'n_estimators': 100}\n",
    "0.889(+/-0.030) for {'n_estimators': 150}\n",
    "0.890(+/-0.029) for {'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8765851011968508\n",
      "0.8765851011968508\n",
      "26173.097349764343\n"
     ]
    }
   ],
   "source": [
    "model_svcRF_t=clf.best_estimator_\n",
    "model_svcRF_t.fit(X_train,y_train)\n",
    "\n",
    "print(model_svcRF_t.score(X_test,y_test))#默认0.4697676007383277\n",
    "\n",
    "y_pred = model_svcRF_t.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)\n",
    "model_svcRF=clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:27:09] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "0.9132798872132086\n",
      "0.9132798872132086\n",
      "21939.728138616727\n"
     ]
    }
   ],
   "source": [
    "model_xgbr = xgb.XGBRegressor(n_estimators=500,max_depth=5,min_child_weight= 3,learning_rate=0.1,random_state=161)\n",
    "model_xgbr.fit(X_train, y_train)\n",
    "\n",
    "print(model_xgbr.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_xgbr.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[15:59:54] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
    "0.8787590706564508\n",
    "0.8787590706564508\n",
    "25941.551890190498\n",
    "\n",
    "21803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:40:31] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "             max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n",
      "             n_jobs=1, nthread=None, objective='reg:linear', random_state=161,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "             silent=None, subsample=1, verbosity=1)\n",
      "0.716(+/-0.024) for {'n_estimators': 20}\n",
      "0.859(+/-0.023) for {'n_estimators': 50}\n",
      "0.889(+/-0.022) for {'n_estimators': 100}\n",
      "0.898(+/-0.020) for {'n_estimators': 150}\n",
      "0.903(+/-0.020) for {'n_estimators': 200}\n",
      "0.908(+/-0.018) for {'n_estimators': 300}\n",
      "0.911(+/-0.018) for {'n_estimators': 400}\n",
      "0.913(+/-0.017) for {'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "#调参 XGboost——n_estimators 其他默认\n",
    "cv_params = [{\"n_estimators\":[20,50,100,150,200,300,400,500]}]\n",
    "\n",
    "scores_GBM=['r2']\n",
    "\n",
    "for score in scores_GBM:\n",
    "    print(score)\n",
    "    model = XGBRegressor(random_state=161)\n",
    "    optimized_GBM=GridSearchCV(estimator=model, param_grid=cv_params, scoring=score, cv=3, verbose=1, n_jobs=-1)\n",
    "    optimized_GBM.fit(X_train,y_train)\n",
    "    \n",
    "    print(optimized_GBM.best_estimator_)\n",
    "    \n",
    "    means_GBM = optimized_GBM.cv_results_['mean_test_score']\n",
    "    stds_GBM=optimized_GBM.cv_results_['std_test_score']\n",
    "    params_GBM = optimized_GBM.cv_results_['params']\n",
    "    \n",
    "    for (param_GBM,mean_score_GBM,std_score_GBM) in zip(params_GBM,means_GBM,stds_GBM):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score_GBM,std_score_GBM,param_GBM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 29.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:59] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "             max_depth=5, min_child_weight=3, missing=None, n_estimators=500,\n",
      "             n_jobs=1, nthread=None, objective='reg:linear', random_state=161,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "             silent=None, subsample=1, verbosity=1)\n",
      "0.913(+/-0.017) for {'max_depth': 3, 'min_child_weight': 1}\n",
      "0.913(+/-0.018) for {'max_depth': 3, 'min_child_weight': 3}\n",
      "0.914(+/-0.022) for {'max_depth': 3, 'min_child_weight': 5}\n",
      "0.917(+/-0.017) for {'max_depth': 5, 'min_child_weight': 1}\n",
      "0.918(+/-0.017) for {'max_depth': 5, 'min_child_weight': 3}\n",
      "0.917(+/-0.020) for {'max_depth': 5, 'min_child_weight': 5}\n",
      "0.913(+/-0.018) for {'max_depth': 7, 'min_child_weight': 1}\n",
      "0.912(+/-0.020) for {'max_depth': 7, 'min_child_weight': 3}\n",
      "0.913(+/-0.021) for {'max_depth': 7, 'min_child_weight': 5}\n",
      "0.908(+/-0.016) for {'max_depth': 9, 'min_child_weight': 1}\n",
      "0.913(+/-0.020) for {'max_depth': 9, 'min_child_weight': 3}\n",
      "0.909(+/-0.022) for {'max_depth': 9, 'min_child_weight': 5}\n"
     ]
    }
   ],
   "source": [
    "#调参 XGboost——maxdepth minchildweight 其他默认\n",
    "cv_params = [{'max_depth': [3, 5,7,9], 'min_child_weight': [1, 3, 5]}]\n",
    "\n",
    "scores_GBM_mami=['r2']\n",
    "\n",
    "for score in scores_GBM_mami:\n",
    "    print(score)\n",
    "    model = optimized_GBM.best_estimator_\n",
    "    optimized_GBM_mami=GridSearchCV(estimator=model, param_grid=cv_params, scoring=score, cv=3, verbose=1, n_jobs=-1)\n",
    "    optimized_GBM_mami.fit(X_train,y_train)\n",
    "    \n",
    "    print(optimized_GBM_mami.best_estimator_)\n",
    "    \n",
    "    means_GBM_mami = optimized_GBM_mami.cv_results_['mean_test_score']\n",
    "    stds_GBM_mami=optimized_GBM_mami.cv_results_['std_test_score']\n",
    "    params_GBM_mami = optimized_GBM_mami.cv_results_['params']\n",
    "    \n",
    "    for (param_GBM_mami,mean_score_GBM_mami,std_score_GBM_mami) in zip(params_GBM_mami,means_GBM_mami,stds_GBM_mami):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score_GBM_mami,std_score_GBM_mami,param_GBM_mami))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed: 12.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:42:08] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "             max_depth=5, min_child_weight=3, missing=None, n_estimators=500,\n",
      "             n_jobs=1, nthread=None, objective='reg:linear', random_state=161,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "             silent=None, subsample=1, verbosity=1)\n",
      "-2.290(+/-0.136) for {'learning_rate': 0.0001}\n",
      "-0.493(+/-0.042) for {'learning_rate': 0.001}\n",
      "0.898(+/-0.022) for {'learning_rate': 0.01}\n",
      "0.918(+/-0.017) for {'learning_rate': 0.1}\n",
      "0.916(+/-0.015) for {'learning_rate': 0.2}\n",
      "0.908(+/-0.023) for {'learning_rate': 0.3}\n"
     ]
    }
   ],
   "source": [
    "#调参 XGboost——学习率\n",
    "cv_params = [{'learning_rate':  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]}]\n",
    "\n",
    "scores_GBM_lr=['r2']\n",
    "\n",
    "for score in scores_GBM_lr:\n",
    "    print(score)\n",
    "    model = optimized_GBM_mami.best_estimator_\n",
    "    optimized_GBM_lr=GridSearchCV(estimator=model, param_grid=cv_params, scoring=score, cv=3, verbose=1, n_jobs=-1)\n",
    "    optimized_GBM_lr.fit(X_train,y_train)\n",
    "    \n",
    "    print(optimized_GBM_lr.best_estimator_)\n",
    "    \n",
    "    means_GBM_lr = optimized_GBM_lr.cv_results_['mean_test_score']\n",
    "    stds_GBM_lr=optimized_GBM_lr.cv_results_['std_test_score']\n",
    "    params_GBM_lr = optimized_GBM_lr.cv_results_['params']\n",
    "    \n",
    "    for (param_GBM_lr,mean_score_GBM_lr,std_score_GBM_lr) in zip(params_GBM_lr,means_GBM_lr,stds_GBM_lr):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score_GBM_lr,std_score_GBM_lr,param_GBM_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgbr_t = optimized_GBM.best_estimator_\n",
    "model_xgbr_t.fit(X_train, y_train)\n",
    "\n",
    "print(model_xgbr_t.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_xgbr_t.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "model_xgbr = optimized_GBM.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7112508524026153\n",
      "0.7112508524026153\n",
      "40034.248464909884\n"
     ]
    }
   ],
   "source": [
    "# 4.kNN回归\n",
    "model_k_neighbor = neighbors.KNeighborsRegressor(n_neighbors=4, p=5,weights='distance')\n",
    "model_k_neighbor.fit(X_train, y_train)\n",
    "\n",
    "print(model_k_neighbor.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_k_neighbor.predict(X_test)\n",
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2\n",
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 83.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                    metric_params=None, n_jobs=None, n_neighbors=4, p=5,\n",
      "                    weights='distance')\n",
      "0.558(+/-0.026) for {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "0.586(+/-0.004) for {'n_neighbors': 2, 'weights': 'uniform'}\n",
      "0.584(+/-0.014) for {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "0.573(+/-0.022) for {'n_neighbors': 4, 'weights': 'uniform'}\n",
      "0.558(+/-0.023) for {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "0.547(+/-0.022) for {'n_neighbors': 6, 'weights': 'uniform'}\n",
      "0.535(+/-0.025) for {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "0.525(+/-0.025) for {'n_neighbors': 8, 'weights': 'uniform'}\n",
      "0.515(+/-0.027) for {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "0.507(+/-0.026) for {'n_neighbors': 10, 'weights': 'uniform'}\n",
      "0.337(+/-0.035) for {'n_neighbors': 1, 'p': 1, 'weights': 'distance'}\n",
      "0.558(+/-0.026) for {'n_neighbors': 1, 'p': 2, 'weights': 'distance'}\n",
      "0.612(+/-0.012) for {'n_neighbors': 1, 'p': 3, 'weights': 'distance'}\n",
      "0.622(+/-0.007) for {'n_neighbors': 1, 'p': 4, 'weights': 'distance'}\n",
      "0.617(+/-0.020) for {'n_neighbors': 1, 'p': 5, 'weights': 'distance'}\n",
      "0.422(+/-0.041) for {'n_neighbors': 2, 'p': 1, 'weights': 'distance'}\n",
      "0.612(+/-0.001) for {'n_neighbors': 2, 'p': 2, 'weights': 'distance'}\n",
      "0.669(+/-0.011) for {'n_neighbors': 2, 'p': 3, 'weights': 'distance'}\n",
      "0.686(+/-0.004) for {'n_neighbors': 2, 'p': 4, 'weights': 'distance'}\n",
      "0.686(+/-0.010) for {'n_neighbors': 2, 'p': 5, 'weights': 'distance'}\n",
      "0.455(+/-0.033) for {'n_neighbors': 3, 'p': 1, 'weights': 'distance'}\n",
      "0.624(+/-0.017) for {'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
      "0.672(+/-0.019) for {'n_neighbors': 3, 'p': 3, 'weights': 'distance'}\n",
      "0.697(+/-0.012) for {'n_neighbors': 3, 'p': 4, 'weights': 'distance'}\n",
      "0.704(+/-0.002) for {'n_neighbors': 3, 'p': 5, 'weights': 'distance'}\n",
      "0.471(+/-0.034) for {'n_neighbors': 4, 'p': 1, 'weights': 'distance'}\n",
      "0.618(+/-0.023) for {'n_neighbors': 4, 'p': 2, 'weights': 'distance'}\n",
      "0.673(+/-0.024) for {'n_neighbors': 4, 'p': 3, 'weights': 'distance'}\n",
      "0.698(+/-0.011) for {'n_neighbors': 4, 'p': 4, 'weights': 'distance'}\n",
      "0.704(+/-0.006) for {'n_neighbors': 4, 'p': 5, 'weights': 'distance'}\n",
      "0.482(+/-0.028) for {'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
      "0.611(+/-0.026) for {'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n",
      "0.669(+/-0.023) for {'n_neighbors': 5, 'p': 3, 'weights': 'distance'}\n",
      "0.695(+/-0.013) for {'n_neighbors': 5, 'p': 4, 'weights': 'distance'}\n",
      "0.699(+/-0.007) for {'n_neighbors': 5, 'p': 5, 'weights': 'distance'}\n",
      "0.489(+/-0.030) for {'n_neighbors': 6, 'p': 1, 'weights': 'distance'}\n",
      "0.605(+/-0.027) for {'n_neighbors': 6, 'p': 2, 'weights': 'distance'}\n",
      "0.664(+/-0.023) for {'n_neighbors': 6, 'p': 3, 'weights': 'distance'}\n",
      "0.689(+/-0.013) for {'n_neighbors': 6, 'p': 4, 'weights': 'distance'}\n",
      "0.690(+/-0.011) for {'n_neighbors': 6, 'p': 5, 'weights': 'distance'}\n",
      "0.490(+/-0.029) for {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}\n",
      "0.597(+/-0.030) for {'n_neighbors': 7, 'p': 2, 'weights': 'distance'}\n",
      "0.657(+/-0.023) for {'n_neighbors': 7, 'p': 3, 'weights': 'distance'}\n",
      "0.679(+/-0.016) for {'n_neighbors': 7, 'p': 4, 'weights': 'distance'}\n",
      "0.682(+/-0.013) for {'n_neighbors': 7, 'p': 5, 'weights': 'distance'}\n",
      "0.493(+/-0.032) for {'n_neighbors': 8, 'p': 1, 'weights': 'distance'}\n",
      "0.590(+/-0.031) for {'n_neighbors': 8, 'p': 2, 'weights': 'distance'}\n",
      "0.652(+/-0.024) for {'n_neighbors': 8, 'p': 3, 'weights': 'distance'}\n",
      "0.671(+/-0.019) for {'n_neighbors': 8, 'p': 4, 'weights': 'distance'}\n",
      "0.672(+/-0.017) for {'n_neighbors': 8, 'p': 5, 'weights': 'distance'}\n",
      "0.495(+/-0.031) for {'n_neighbors': 9, 'p': 1, 'weights': 'distance'}\n",
      "0.582(+/-0.033) for {'n_neighbors': 9, 'p': 2, 'weights': 'distance'}\n",
      "0.643(+/-0.025) for {'n_neighbors': 9, 'p': 3, 'weights': 'distance'}\n",
      "0.662(+/-0.020) for {'n_neighbors': 9, 'p': 4, 'weights': 'distance'}\n",
      "0.665(+/-0.018) for {'n_neighbors': 9, 'p': 5, 'weights': 'distance'}\n",
      "0.496(+/-0.034) for {'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "0.577(+/-0.033) for {'n_neighbors': 10, 'p': 2, 'weights': 'distance'}\n",
      "0.636(+/-0.024) for {'n_neighbors': 10, 'p': 3, 'weights': 'distance'}\n",
      "0.652(+/-0.021) for {'n_neighbors': 10, 'p': 4, 'weights': 'distance'}\n",
      "0.655(+/-0.019) for {'n_neighbors': 10, 'p': 5, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "param_grids=[\n",
    "    {\n",
    "        'weights':['uniform'],\n",
    "        'n_neighbors':[i for i in range(1,11)]\n",
    "    },\n",
    "    {\n",
    "        'weights':['distance'],\n",
    "        'n_neighbors':[i for i in range(1,11)],\n",
    "        'p':[i for i in range(1,6)]\n",
    "    }]\n",
    "scores_KNN=['r2']\n",
    "\n",
    "for score in scores_KNN:\n",
    "    print(score)\n",
    "    model = neighbors.KNeighborsRegressor()\n",
    "    clf_k=GridSearchCV(estimator=model, param_grid=param_grids, scoring=score, cv=3, verbose=1, n_jobs=-1)\n",
    "    clf_k.fit(X_train,y_train)\n",
    "    \n",
    "    print(clf_k.best_estimator_)\n",
    "    \n",
    "    means_KNN = clf_k.cv_results_['mean_test_score']\n",
    "    stds_KNN=clf_k.cv_results_['std_test_score']\n",
    "    params_KNN = clf_k.cv_results_['params']\n",
    "    \n",
    "    for (param_KNN,mean_score_KNN,std_score_KNN) in zip(params_KNN,means_KNN,stds_KNN):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score_KNN,std_score_KNN,param_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k_neighbor_t = clf_k.best_estimator_\n",
    "model_k_neighbor_t.fit(X_train, y_train)\n",
    "\n",
    "print(model_k_neighbor_t.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_k_neighbor_t.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "\n",
    "model_k_neighbor = clf_k.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9014748261569127\n",
      "0.9014748261569127\n",
      "23385.40683455488\n"
     ]
    }
   ],
   "source": [
    "model_gradient_boosting_regressor = ensemble.GradientBoostingRegressor(n_estimators=500,random_state=162)  \n",
    "\n",
    "model_gradient_boosting_regressor.fit(X_train, y_train)\n",
    "\n",
    "print(model_gradient_boosting_regressor.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_gradient_boosting_regressor.predict(X_test)\n",
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "                          learning_rate=0.1, loss='ls', max_depth=3,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                          n_iter_no_change=None, presort='auto',\n",
      "                          random_state=162, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "0.758(+/-0.025) for {'n_estimators': 20}\n",
      "0.859(+/-0.025) for {'n_estimators': 50}\n",
      "0.888(+/-0.024) for {'n_estimators': 100}\n",
      "0.897(+/-0.023) for {'n_estimators': 150}\n",
      "0.901(+/-0.021) for {'n_estimators': 200}\n",
      "0.904(+/-0.019) for {'n_estimators': 300}\n",
      "0.907(+/-0.018) for {'n_estimators': 400}\n",
      "0.909(+/-0.019) for {'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "N_estimators=[{\"n_estimators\":[20,50,100,150,200,300,400,500]}]\n",
    "scores_g=['r2']\n",
    "for score in scores_g:\n",
    "    print(score)\n",
    "    clf_gb=GridSearchCV(ensemble.GradientBoostingRegressor(random_state=162),param_grid=N_estimators,cv=3,scoring=score)\n",
    "    clf_gb.fit(X_train,y_train)\n",
    "    \n",
    "    print(clf_gb.best_estimator_)\n",
    "    \n",
    "    means = clf_gb.cv_results_['mean_test_score']\n",
    "    stds=clf_gb.cv_results_['std_test_score']\n",
    "    params = clf_gb.cv_results_['params']\n",
    "    \n",
    "    for (param,mean_score,std_score) in zip(params,means,stds):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score,std_score,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gradient_boosting_regressor_t = clf_gb.best_estimator_\n",
    "\n",
    "model_gradient_boosting_regressor_t.fit(X_train, y_train)\n",
    "\n",
    "print(model_gradient_boosting_regressor_t.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_gradient_boosting_regressor_t.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "\n",
    "model_gradient_boosting_regressor = clf_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二阶段stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,\n",
      "          fit_path=True, max_iter=500, normalize=True, positive=False,\n",
      "          precompute='auto', verbose=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "# '''创建训练的数据集'''\n",
    "#data, target = make_blobs(n_samples=50000, centers=2, random_state=0, cluster_std=0.60)\n",
    " \n",
    "# '''模型融合中使用到的各个单模型'''\n",
    "# clfs = [RandomForestRegressor(n_estimators=200,random_state=160),\n",
    "#         xgb.XGBRegressor(n_estimators=500,max_depth=5,min_child_weight= 3,learning_rate=0.1,random_state=161),\n",
    "#         neighbors.KNeighborsRegressor(n_neighbors=4, p=5,weights='distance'),\n",
    "#         ensemble.GradientBoostingRegressor(n_estimators=500,random_state=162)\n",
    "#        ]\n",
    "\n",
    "clfs=[linear_model.LassoLars()]\n",
    " \n",
    "#'''切分一部分数据作为测试集'''\n",
    "X=X#训练集 数据\n",
    "X_predict=X_predict#测试集 数据\n",
    "y=score_train#训练集 分数\n",
    "#y_predict = \n",
    "\n",
    "\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))#第一轮 保存各个模型在训练集上的预测结果 训练集合个数×模型数\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))#第一轮 保存各个模型在测试集上的预测结果 训练集合个数×模型数\n",
    "\n",
    "#'''5折stacking'''\n",
    "n_folds = 5\n",
    "kf = KFold(n_folds,True,22)\n",
    "skf=list(kf.split(X))#X或者y\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    #'''依次训练各个单模型'''\n",
    "    print(j, clf)\n",
    "    dataset_blend_test_j = np.zeros((X_predict.shape[0], len(skf)))#存目前这个模型上的测试集结果(之后求平均)\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        #'''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
    "        print(\"Fold\", i)\n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict(X_test)#1fold的预测结果\n",
    "        y_submission=y_submission.flatten()#2维数组转1维 KNN需要 \n",
    "        #y_submission.reshape(len(y_submission),1)#一维数组转二维 可以不加\n",
    "        \n",
    "        dataset_blend_train[test, j] = y_submission#在模型顺序对应的j位置 存1fold的预测结果\n",
    "        dataset_blend_test_j[:, i] = clf.predict(X_predict).flatten()#存该模型该折下的测试集预测结果\n",
    "        \n",
    "    #'''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)#测试集结果按行取平均后储存\n",
    "    \n",
    "    \n",
    "    #print(\"val auc Score: %f\" % r2_score(y_predict, dataset_blend_test[:, j]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导出数据集到本地\n",
    "submission_train_1=pd.DataFrame(dataset_blend_train)\n",
    "#submission_train_1.head()\n",
    "submission_train_1.to_csv('dataset_blend_train.csv',index=False)#第一轮训练后 train集合预测得到的score集合 训练集样本数x3个模型\n",
    "\n",
    "submission_test_1=pd.DataFrame(dataset_blend_test)\n",
    "#submission_test_1.head()\n",
    "submission_test_1.to_csv('dataset_blend_test.csv',index=False)#第一轮训练后 test集合预测得到的score集合 测试机样本数x3个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 69767.87412296,  67766.24768285],\n",
       "       [ 73157.26412891,  72440.8071711 ],\n",
       "       [138781.99988668, 138958.9755904 ],\n",
       "       ...,\n",
       "       [143100.38845517, 142856.80655515],\n",
       "       [ 97637.71248373,  98126.75807216],\n",
       "       [ 66485.08161537,  68580.36251023]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_blend_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89937.500000</td>\n",
       "      <td>84493.390625</td>\n",
       "      <td>108429.109267</td>\n",
       "      <td>79433.521061</td>\n",
       "      <td>69767.874123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62443.650000</td>\n",
       "      <td>69395.703125</td>\n",
       "      <td>64634.334669</td>\n",
       "      <td>74665.063106</td>\n",
       "      <td>73157.264129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106437.500000</td>\n",
       "      <td>106312.164062</td>\n",
       "      <td>120705.589000</td>\n",
       "      <td>108668.400088</td>\n",
       "      <td>138781.999887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161030.000000</td>\n",
       "      <td>165868.609375</td>\n",
       "      <td>132800.298222</td>\n",
       "      <td>167444.237734</td>\n",
       "      <td>174284.761975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69494.600000</td>\n",
       "      <td>68470.320312</td>\n",
       "      <td>50971.584571</td>\n",
       "      <td>64731.329705</td>\n",
       "      <td>69556.103052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>177290.500000</td>\n",
       "      <td>158107.656250</td>\n",
       "      <td>115982.129181</td>\n",
       "      <td>159979.370161</td>\n",
       "      <td>138019.289778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>83135.000000</td>\n",
       "      <td>80776.789062</td>\n",
       "      <td>63887.549592</td>\n",
       "      <td>78928.931018</td>\n",
       "      <td>70819.427851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>121170.000000</td>\n",
       "      <td>126021.359375</td>\n",
       "      <td>115000.000000</td>\n",
       "      <td>130665.155413</td>\n",
       "      <td>125717.205029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>159016.000000</td>\n",
       "      <td>171063.640625</td>\n",
       "      <td>89336.987986</td>\n",
       "      <td>164552.544890</td>\n",
       "      <td>180415.592093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73365.000000</td>\n",
       "      <td>75764.335938</td>\n",
       "      <td>74780.450806</td>\n",
       "      <td>75330.402797</td>\n",
       "      <td>73525.936277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>103777.500000</td>\n",
       "      <td>97343.757812</td>\n",
       "      <td>105000.000000</td>\n",
       "      <td>97162.083631</td>\n",
       "      <td>101284.431976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>56000.500000</td>\n",
       "      <td>47122.960938</td>\n",
       "      <td>53405.418831</td>\n",
       "      <td>47166.392633</td>\n",
       "      <td>30910.765149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>203573.333333</td>\n",
       "      <td>193006.906250</td>\n",
       "      <td>215242.691767</td>\n",
       "      <td>202866.982810</td>\n",
       "      <td>216387.607792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>121043.000000</td>\n",
       "      <td>119407.140625</td>\n",
       "      <td>77226.047362</td>\n",
       "      <td>107773.840731</td>\n",
       "      <td>113904.981240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70900.000000</td>\n",
       "      <td>68094.945312</td>\n",
       "      <td>90919.106980</td>\n",
       "      <td>67615.138898</td>\n",
       "      <td>58140.410016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>95372.415000</td>\n",
       "      <td>103107.773438</td>\n",
       "      <td>99098.739093</td>\n",
       "      <td>112011.063167</td>\n",
       "      <td>133290.691241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>89037.333333</td>\n",
       "      <td>92610.656250</td>\n",
       "      <td>81992.987170</td>\n",
       "      <td>86637.822661</td>\n",
       "      <td>89148.456696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>72388.000000</td>\n",
       "      <td>72146.609375</td>\n",
       "      <td>74075.870102</td>\n",
       "      <td>72066.864362</td>\n",
       "      <td>65920.327166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>251545.400000</td>\n",
       "      <td>308322.156250</td>\n",
       "      <td>109296.149271</td>\n",
       "      <td>279804.969916</td>\n",
       "      <td>250391.821971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>137651.635000</td>\n",
       "      <td>178631.390625</td>\n",
       "      <td>263480.099014</td>\n",
       "      <td>177192.906720</td>\n",
       "      <td>171465.051149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>150608.155000</td>\n",
       "      <td>152370.625000</td>\n",
       "      <td>87887.525379</td>\n",
       "      <td>127117.153386</td>\n",
       "      <td>160525.397559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>83727.500000</td>\n",
       "      <td>90778.031250</td>\n",
       "      <td>125039.286941</td>\n",
       "      <td>88532.890133</td>\n",
       "      <td>103404.258309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>313753.500000</td>\n",
       "      <td>313871.312500</td>\n",
       "      <td>252880.755191</td>\n",
       "      <td>307617.129689</td>\n",
       "      <td>286108.055275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>84075.000000</td>\n",
       "      <td>85069.960938</td>\n",
       "      <td>82788.678949</td>\n",
       "      <td>78039.138711</td>\n",
       "      <td>83520.846745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>48617.800000</td>\n",
       "      <td>41374.640625</td>\n",
       "      <td>61032.530542</td>\n",
       "      <td>42439.601960</td>\n",
       "      <td>12277.384044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>60702.500000</td>\n",
       "      <td>58270.972656</td>\n",
       "      <td>62928.142862</td>\n",
       "      <td>63981.465498</td>\n",
       "      <td>59136.284891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>169556.850000</td>\n",
       "      <td>178654.640625</td>\n",
       "      <td>131900.143263</td>\n",
       "      <td>192075.989730</td>\n",
       "      <td>176717.157964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>44857.500000</td>\n",
       "      <td>45837.664062</td>\n",
       "      <td>56879.890539</td>\n",
       "      <td>43094.072713</td>\n",
       "      <td>15383.050631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>139270.000000</td>\n",
       "      <td>146345.437500</td>\n",
       "      <td>90410.456891</td>\n",
       "      <td>148419.408558</td>\n",
       "      <td>144853.999515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>119033.500000</td>\n",
       "      <td>126917.781250</td>\n",
       "      <td>123138.504759</td>\n",
       "      <td>125555.340204</td>\n",
       "      <td>153307.423059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31440</th>\n",
       "      <td>136980.000000</td>\n",
       "      <td>128200.484375</td>\n",
       "      <td>98959.570729</td>\n",
       "      <td>123430.925384</td>\n",
       "      <td>119987.504002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31441</th>\n",
       "      <td>107805.000000</td>\n",
       "      <td>101432.890625</td>\n",
       "      <td>121969.354259</td>\n",
       "      <td>95955.667686</td>\n",
       "      <td>101368.355988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31442</th>\n",
       "      <td>65405.000000</td>\n",
       "      <td>77509.218750</td>\n",
       "      <td>57583.518039</td>\n",
       "      <td>77101.240581</td>\n",
       "      <td>44265.458388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31443</th>\n",
       "      <td>235769.500000</td>\n",
       "      <td>221171.484375</td>\n",
       "      <td>206738.301630</td>\n",
       "      <td>250027.718563</td>\n",
       "      <td>245139.521395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31444</th>\n",
       "      <td>88222.000000</td>\n",
       "      <td>89187.507812</td>\n",
       "      <td>107649.750402</td>\n",
       "      <td>83596.752804</td>\n",
       "      <td>81724.017975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31445</th>\n",
       "      <td>114147.230000</td>\n",
       "      <td>105459.703125</td>\n",
       "      <td>75653.924367</td>\n",
       "      <td>119157.052411</td>\n",
       "      <td>147215.473398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31446</th>\n",
       "      <td>94515.000000</td>\n",
       "      <td>91695.218750</td>\n",
       "      <td>93435.432840</td>\n",
       "      <td>93906.350340</td>\n",
       "      <td>99845.963040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31447</th>\n",
       "      <td>133620.000000</td>\n",
       "      <td>161736.781250</td>\n",
       "      <td>129101.854034</td>\n",
       "      <td>160510.087506</td>\n",
       "      <td>198451.851935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31448</th>\n",
       "      <td>57130.000000</td>\n",
       "      <td>51129.238281</td>\n",
       "      <td>53815.778239</td>\n",
       "      <td>49780.351085</td>\n",
       "      <td>26955.585369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31449</th>\n",
       "      <td>61691.500000</td>\n",
       "      <td>63571.031250</td>\n",
       "      <td>69548.466979</td>\n",
       "      <td>64449.147052</td>\n",
       "      <td>47409.030512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31450</th>\n",
       "      <td>121421.750000</td>\n",
       "      <td>126662.890625</td>\n",
       "      <td>103955.209559</td>\n",
       "      <td>121866.835362</td>\n",
       "      <td>132549.589871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31451</th>\n",
       "      <td>218987.500000</td>\n",
       "      <td>217794.546875</td>\n",
       "      <td>211964.792636</td>\n",
       "      <td>217423.684262</td>\n",
       "      <td>242806.970239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31452</th>\n",
       "      <td>89303.550000</td>\n",
       "      <td>92698.281250</td>\n",
       "      <td>112029.204081</td>\n",
       "      <td>95117.068515</td>\n",
       "      <td>106363.846342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31453</th>\n",
       "      <td>48126.000000</td>\n",
       "      <td>48142.019531</td>\n",
       "      <td>49569.354418</td>\n",
       "      <td>44999.510339</td>\n",
       "      <td>28944.163490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31454</th>\n",
       "      <td>111850.500000</td>\n",
       "      <td>111985.382812</td>\n",
       "      <td>124692.311074</td>\n",
       "      <td>113706.181211</td>\n",
       "      <td>127239.190821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31455</th>\n",
       "      <td>79784.500000</td>\n",
       "      <td>73872.539062</td>\n",
       "      <td>57500.969655</td>\n",
       "      <td>87551.982774</td>\n",
       "      <td>103427.604959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31456</th>\n",
       "      <td>136378.000000</td>\n",
       "      <td>151666.968750</td>\n",
       "      <td>139000.000000</td>\n",
       "      <td>165239.956913</td>\n",
       "      <td>187375.970670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31457</th>\n",
       "      <td>71860.000000</td>\n",
       "      <td>64090.378906</td>\n",
       "      <td>80475.465743</td>\n",
       "      <td>69848.960693</td>\n",
       "      <td>54319.041830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31458</th>\n",
       "      <td>81452.741667</td>\n",
       "      <td>80216.210938</td>\n",
       "      <td>109289.749110</td>\n",
       "      <td>82541.278023</td>\n",
       "      <td>93268.756963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31459</th>\n",
       "      <td>106595.000000</td>\n",
       "      <td>114514.757812</td>\n",
       "      <td>91365.257611</td>\n",
       "      <td>103595.286233</td>\n",
       "      <td>125948.458772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31460</th>\n",
       "      <td>185207.500000</td>\n",
       "      <td>193720.750000</td>\n",
       "      <td>239693.287222</td>\n",
       "      <td>209274.714751</td>\n",
       "      <td>278496.066029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31461</th>\n",
       "      <td>114609.000000</td>\n",
       "      <td>130598.617188</td>\n",
       "      <td>210052.621268</td>\n",
       "      <td>128987.327621</td>\n",
       "      <td>166559.327203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31462</th>\n",
       "      <td>247466.000000</td>\n",
       "      <td>240963.781250</td>\n",
       "      <td>224389.801932</td>\n",
       "      <td>248881.679194</td>\n",
       "      <td>226664.600788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31463</th>\n",
       "      <td>249189.500000</td>\n",
       "      <td>253664.218750</td>\n",
       "      <td>248000.000000</td>\n",
       "      <td>255496.313174</td>\n",
       "      <td>218349.181001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31464</th>\n",
       "      <td>103762.500000</td>\n",
       "      <td>107487.851562</td>\n",
       "      <td>93732.444038</td>\n",
       "      <td>108312.987998</td>\n",
       "      <td>113956.487291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31465</th>\n",
       "      <td>79953.000000</td>\n",
       "      <td>75455.914062</td>\n",
       "      <td>83384.681104</td>\n",
       "      <td>77309.202435</td>\n",
       "      <td>78789.661907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31466</th>\n",
       "      <td>56710.000000</td>\n",
       "      <td>58332.929688</td>\n",
       "      <td>94149.482048</td>\n",
       "      <td>63389.512432</td>\n",
       "      <td>55186.361153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31467</th>\n",
       "      <td>126028.333333</td>\n",
       "      <td>115796.414062</td>\n",
       "      <td>125000.000000</td>\n",
       "      <td>122235.418636</td>\n",
       "      <td>143100.388455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31468</th>\n",
       "      <td>94852.666667</td>\n",
       "      <td>93706.734375</td>\n",
       "      <td>96473.139323</td>\n",
       "      <td>94391.437391</td>\n",
       "      <td>97637.712484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31469</th>\n",
       "      <td>70431.000000</td>\n",
       "      <td>71720.328125</td>\n",
       "      <td>72939.712278</td>\n",
       "      <td>67543.827343</td>\n",
       "      <td>66485.081615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31470 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1              2              3  \\\n",
       "0       89937.500000   84493.390625  108429.109267   79433.521061   \n",
       "1       62443.650000   69395.703125   64634.334669   74665.063106   \n",
       "2      106437.500000  106312.164062  120705.589000  108668.400088   \n",
       "3      161030.000000  165868.609375  132800.298222  167444.237734   \n",
       "4       69494.600000   68470.320312   50971.584571   64731.329705   \n",
       "...              ...            ...            ...            ...   \n",
       "31465   79953.000000   75455.914062   83384.681104   77309.202435   \n",
       "31466   56710.000000   58332.929688   94149.482048   63389.512432   \n",
       "31467  126028.333333  115796.414062  125000.000000  122235.418636   \n",
       "31468   94852.666667   93706.734375   96473.139323   94391.437391   \n",
       "31469   70431.000000   71720.328125   72939.712278   67543.827343   \n",
       "\n",
       "                   0  \n",
       "0       69767.874123  \n",
       "1       73157.264129  \n",
       "2      138781.999887  \n",
       "3      174284.761975  \n",
       "4       69556.103052  \n",
       "...              ...  \n",
       "31465   78789.661907  \n",
       "31466   55186.361153  \n",
       "31467  143100.388455  \n",
       "31468   97637.712484  \n",
       "31469   66485.081615  \n",
       "\n",
       "[31470 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155050.600000</td>\n",
       "      <td>145980.575000</td>\n",
       "      <td>105068.387634</td>\n",
       "      <td>149208.125516</td>\n",
       "      <td>69767.874123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114510.900000</td>\n",
       "      <td>115455.056250</td>\n",
       "      <td>121635.602127</td>\n",
       "      <td>116022.071633</td>\n",
       "      <td>73157.264129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81643.404333</td>\n",
       "      <td>77090.054688</td>\n",
       "      <td>84493.311487</td>\n",
       "      <td>75629.253182</td>\n",
       "      <td>138781.999887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59682.300000</td>\n",
       "      <td>60101.988281</td>\n",
       "      <td>61453.743495</td>\n",
       "      <td>60992.329779</td>\n",
       "      <td>174284.761975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120968.511905</td>\n",
       "      <td>127010.950000</td>\n",
       "      <td>95323.079347</td>\n",
       "      <td>132652.352853</td>\n",
       "      <td>69556.103052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>66154.466667</td>\n",
       "      <td>72803.907812</td>\n",
       "      <td>100329.761211</td>\n",
       "      <td>69111.170237</td>\n",
       "      <td>138019.289778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>94263.400000</td>\n",
       "      <td>89360.315625</td>\n",
       "      <td>117596.418753</td>\n",
       "      <td>99250.824806</td>\n",
       "      <td>70819.427851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>89537.800000</td>\n",
       "      <td>91857.209375</td>\n",
       "      <td>73326.588462</td>\n",
       "      <td>91624.620988</td>\n",
       "      <td>125717.205029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>94743.150000</td>\n",
       "      <td>94670.592188</td>\n",
       "      <td>66202.089772</td>\n",
       "      <td>91200.873716</td>\n",
       "      <td>180415.592093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>133978.400000</td>\n",
       "      <td>122991.323438</td>\n",
       "      <td>109126.435271</td>\n",
       "      <td>126428.406598</td>\n",
       "      <td>73525.936277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>118525.566667</td>\n",
       "      <td>103010.198438</td>\n",
       "      <td>105842.445408</td>\n",
       "      <td>117496.506705</td>\n",
       "      <td>101284.431976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>49805.800000</td>\n",
       "      <td>52712.158594</td>\n",
       "      <td>70495.999129</td>\n",
       "      <td>51476.689796</td>\n",
       "      <td>30910.765149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>120805.020000</td>\n",
       "      <td>127339.715625</td>\n",
       "      <td>113370.272251</td>\n",
       "      <td>130354.010462</td>\n",
       "      <td>216387.607792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>57785.616667</td>\n",
       "      <td>55672.966406</td>\n",
       "      <td>68531.962102</td>\n",
       "      <td>54119.420408</td>\n",
       "      <td>113904.981240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>101475.400000</td>\n",
       "      <td>99829.109375</td>\n",
       "      <td>100934.046392</td>\n",
       "      <td>100073.537327</td>\n",
       "      <td>58140.410016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>49811.466667</td>\n",
       "      <td>48405.908594</td>\n",
       "      <td>69104.814510</td>\n",
       "      <td>50191.921036</td>\n",
       "      <td>133290.691241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>119924.400000</td>\n",
       "      <td>117422.017187</td>\n",
       "      <td>132624.916149</td>\n",
       "      <td>119361.023115</td>\n",
       "      <td>89148.456696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>541586.333333</td>\n",
       "      <td>555018.300000</td>\n",
       "      <td>471524.285005</td>\n",
       "      <td>545998.499631</td>\n",
       "      <td>65920.327166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>98697.666667</td>\n",
       "      <td>105445.282812</td>\n",
       "      <td>85878.901130</td>\n",
       "      <td>101330.139129</td>\n",
       "      <td>250391.821971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>37457.460000</td>\n",
       "      <td>34141.945312</td>\n",
       "      <td>64499.103911</td>\n",
       "      <td>30675.489315</td>\n",
       "      <td>171465.051149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>127031.926667</td>\n",
       "      <td>136169.153125</td>\n",
       "      <td>153450.404714</td>\n",
       "      <td>130498.358667</td>\n",
       "      <td>160525.397559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>144185.000000</td>\n",
       "      <td>124308.068750</td>\n",
       "      <td>118057.473536</td>\n",
       "      <td>125463.522751</td>\n",
       "      <td>103404.258309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>95816.940000</td>\n",
       "      <td>104097.506250</td>\n",
       "      <td>109116.241349</td>\n",
       "      <td>103924.143761</td>\n",
       "      <td>286108.055275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>139580.100000</td>\n",
       "      <td>126715.593750</td>\n",
       "      <td>127453.789225</td>\n",
       "      <td>136851.146571</td>\n",
       "      <td>83520.846745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>145974.350000</td>\n",
       "      <td>150634.271875</td>\n",
       "      <td>121542.291417</td>\n",
       "      <td>152127.779554</td>\n",
       "      <td>12277.384044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>276995.813333</td>\n",
       "      <td>281241.506250</td>\n",
       "      <td>256750.965781</td>\n",
       "      <td>262875.290365</td>\n",
       "      <td>59136.284891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>200343.000000</td>\n",
       "      <td>200436.156250</td>\n",
       "      <td>253428.189060</td>\n",
       "      <td>205132.347626</td>\n",
       "      <td>176717.157964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>83962.800000</td>\n",
       "      <td>87132.067187</td>\n",
       "      <td>81949.530542</td>\n",
       "      <td>92172.105102</td>\n",
       "      <td>15383.050631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>63468.640000</td>\n",
       "      <td>63854.186719</td>\n",
       "      <td>65629.378965</td>\n",
       "      <td>65871.340420</td>\n",
       "      <td>144853.999515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>114304.679333</td>\n",
       "      <td>113033.440625</td>\n",
       "      <td>104540.716665</td>\n",
       "      <td>118223.820708</td>\n",
       "      <td>153307.423059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31440</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119987.504002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31441</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101368.355988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31442</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44265.458388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31443</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>245139.521395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31444</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81724.017975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31445</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147215.473398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31446</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99845.963040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31447</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>198451.851935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31448</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26955.585369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31449</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47409.030512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31450</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132549.589871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31451</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>242806.970239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31452</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106363.846342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31453</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28944.163490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31454</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127239.190821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31455</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103427.604959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31456</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187375.970670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31457</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54319.041830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31458</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93268.756963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31459</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125948.458772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31460</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278496.066029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31461</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166559.327203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31462</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>226664.600788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31463</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>218349.181001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31464</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113956.487291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31465</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78789.661907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31466</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55186.361153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31467</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143100.388455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31468</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97637.712484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31469</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66485.081615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31470 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1              2              3  \\\n",
       "0      155050.600000  145980.575000  105068.387634  149208.125516   \n",
       "1      114510.900000  115455.056250  121635.602127  116022.071633   \n",
       "2       81643.404333   77090.054688   84493.311487   75629.253182   \n",
       "3       59682.300000   60101.988281   61453.743495   60992.329779   \n",
       "4      120968.511905  127010.950000   95323.079347  132652.352853   \n",
       "...              ...            ...            ...            ...   \n",
       "31465            NaN            NaN            NaN            NaN   \n",
       "31466            NaN            NaN            NaN            NaN   \n",
       "31467            NaN            NaN            NaN            NaN   \n",
       "31468            NaN            NaN            NaN            NaN   \n",
       "31469            NaN            NaN            NaN            NaN   \n",
       "\n",
       "                   0  \n",
       "0       69767.874123  \n",
       "1       73157.264129  \n",
       "2      138781.999887  \n",
       "3      174284.761975  \n",
       "4       69556.103052  \n",
       "...              ...  \n",
       "31465   78789.661907  \n",
       "31466   55186.361153  \n",
       "31467  143100.388455  \n",
       "31468   97637.712484  \n",
       "31469   66485.081615  \n",
       "\n",
       "[31470 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_train_1=pd.read_csv('dataset_blend_train.csv')\n",
    "submission_test_1=pd.read_csv('dataset_blend_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=pd.DataFrame(dataset_blend_train)\n",
    "submission_train_1=pd.concat([submission_train_1,a],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=pd.DataFrame(dataset_blend_test)\n",
    "submission_test_1=pd.concat([submission_test_1,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_blend_train=submission_train_1.values\n",
    "dataset_blend_test=submission_test_1.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、予測（predict）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:42:40] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "#第二轮 \n",
    "model_stacking_xgb = optimized_GBM.best_estimator_##xgb.XGBRegressor(random_state=200)\n",
    "model_stacking_xgb.fit(dataset_blend_train, y)\n",
    "\n",
    "\n",
    "#model_stacking_svm=svm.SVR()\n",
    "#model_stacking_svm.fit(dataset_blend_train, y)\n",
    "\n",
    "y_submission = model_stacking_xgb.predict(dataset_blend_test)\n",
    "\n",
    "#生成文件\n",
    "submission_df=pd.DataFrame(data={'id':id_test,'SalePrice':y_submission})\n",
    "submission_df.head()\n",
    "submission_df.to_csv('baseline_2_v.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25176, 5) (25176,) (6294, 5) (6294,)\n",
      "r2\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   14.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:42:09] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "             max_depth=3, min_child_weight=1, missing=None, n_estimators=50,\n",
      "             n_jobs=1, nthread=None, objective='reg:linear', random_state=161,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "             silent=None, subsample=1, verbosity=1)\n",
      "0.854(+/-0.015) for {'n_estimators': 20}\n",
      "0.913(+/-0.016) for {'n_estimators': 50}\n",
      "0.913(+/-0.018) for {'n_estimators': 100}\n",
      "0.911(+/-0.018) for {'n_estimators': 150}\n",
      "0.911(+/-0.019) for {'n_estimators': 200}\n",
      "0.911(+/-0.019) for {'n_estimators': 300}\n",
      "0.910(+/-0.019) for {'n_estimators': 400}\n",
      "0.910(+/-0.019) for {'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "#调参数\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_blend_train, y, test_size=0.2, random_state=123)\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)\n",
    "\n",
    "#调参 XGboost——n_estimators 其他默认\n",
    "cv_params = [{\"n_estimators\":[20,50,100,150,200,300,400,500]}]\n",
    "\n",
    "scores_GBM=['r2']\n",
    "\n",
    "for score in scores_GBM:\n",
    "    print(score)\n",
    "    model = XGBRegressor(random_state=161)\n",
    "    optimized_GBM=GridSearchCV(estimator=model, param_grid=cv_params, scoring=score, cv=3, verbose=1, n_jobs=-1)\n",
    "    optimized_GBM.fit(X_train,y_train)\n",
    "    \n",
    "    print(optimized_GBM.best_estimator_)\n",
    "    \n",
    "    means_GBM = optimized_GBM.cv_results_['mean_test_score']\n",
    "    stds_GBM=optimized_GBM.cv_results_['std_test_score']\n",
    "    params_GBM = optimized_GBM.cv_results_['params']\n",
    "    \n",
    "    for (param_GBM,mean_score_GBM,std_score_GBM) in zip(params_GBM,means_GBM,stds_GBM):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score_GBM,std_score_GBM,param_GBM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:42:18] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "             max_depth=5, min_child_weight=3, missing=None, n_estimators=50,\n",
      "             n_jobs=1, nthread=None, objective='reg:linear', random_state=161,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "             silent=None, subsample=1, verbosity=1)\n",
      "0.913(+/-0.016) for {'max_depth': 3, 'min_child_weight': 1}\n",
      "0.914(+/-0.009) for {'max_depth': 3, 'min_child_weight': 3}\n",
      "0.912(+/-0.016) for {'max_depth': 3, 'min_child_weight': 5}\n",
      "0.912(+/-0.018) for {'max_depth': 5, 'min_child_weight': 1}\n",
      "0.914(+/-0.008) for {'max_depth': 5, 'min_child_weight': 3}\n",
      "0.911(+/-0.018) for {'max_depth': 5, 'min_child_weight': 5}\n",
      "0.910(+/-0.018) for {'max_depth': 7, 'min_child_weight': 1}\n",
      "0.913(+/-0.008) for {'max_depth': 7, 'min_child_weight': 3}\n",
      "0.909(+/-0.018) for {'max_depth': 7, 'min_child_weight': 5}\n",
      "0.902(+/-0.022) for {'max_depth': 9, 'min_child_weight': 1}\n",
      "0.913(+/-0.008) for {'max_depth': 9, 'min_child_weight': 3}\n",
      "0.907(+/-0.017) for {'max_depth': 9, 'min_child_weight': 5}\n"
     ]
    }
   ],
   "source": [
    "#调参 XGboost——maxdepth minchildweight 其他默认\n",
    "cv_params = [{'max_depth': [3, 5,7,9], 'min_child_weight': [1, 3, 5]}]\n",
    "\n",
    "scores_GBM_mami=['r2']\n",
    "\n",
    "for score in scores_GBM_mami:\n",
    "    print(score)\n",
    "    model = optimized_GBM.best_estimator_\n",
    "    optimized_GBM_mami=GridSearchCV(estimator=model, param_grid=cv_params, scoring=score, cv=3, verbose=1, n_jobs=-1)\n",
    "    optimized_GBM_mami.fit(X_train,y_train)\n",
    "    \n",
    "    print(optimized_GBM_mami.best_estimator_)\n",
    "    \n",
    "    means_GBM_mami = optimized_GBM_mami.cv_results_['mean_test_score']\n",
    "    stds_GBM_mami=optimized_GBM_mami.cv_results_['std_test_score']\n",
    "    params_GBM_mami = optimized_GBM_mami.cv_results_['params']\n",
    "    \n",
    "    for (param_GBM_mami,mean_score_GBM_mami,std_score_GBM_mami) in zip(params_GBM_mami,means_GBM_mami,stds_GBM_mami):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score_GBM_mami,std_score_GBM_mami,param_GBM_mami))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25176, 4) (25176,) (6294, 4) (6294,)\n",
      "[14:32:51] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "0.9217693680580265\n",
      "0.9217693680580265\n",
      "0.9217693680580265\n",
      "21282.752056579666\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset_blend_train, y, test_size=0.2, random_state=156)\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)\n",
    "\n",
    "model_xgbr_t = optimized_GBM.best_estimator_\n",
    "model_xgbr_t.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(model_xgbr_t.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_xgbr_t.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "model_xgbr = optimized_GBM.best_estimator_\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test,y_pred))\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#肯定不准确的预测score\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_blend_train, y, test_size=0.2, random_state=156)\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)\n",
    "\n",
    "model_stacking_svm.fit(X_train, y_train)\n",
    "\n",
    "print(model_stacking_svm.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_st_best = clf_st.best_estimator_\n",
    "\n",
    "clf_st_best.fit(dataset_blend_train, y)\n",
    "y_submission = clf_st_best.predict(dataset_blend_test)\n",
    "\n",
    "#生成文件\n",
    "submission_df=pd.DataFrame(data={'Id':id_test,'SalePrice':y_submission})\n",
    "submission_df.head()\n",
    "submission_df.to_csv('baseline_st3_2.csv',header=False,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
